{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installing Libraries for LangChain and Supporting Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjDmS3Cn2jKm",
        "outputId": "569cfc27-0dbb-455a-85cb-3a75b57bbbf2"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install faiss-gpu\n",
        "!pip install langchain_experimental\n",
        "!pip install \"langchain[docarray]\"\n",
        "!pip install pylcs\n",
        "!pip3 install pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing Necessary Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIL5OCqJ7bh8"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import  PyPDFLoader\n",
        "from langchain.vectorstores import  FAISS\n",
        "from langchain.text_splitter import  RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings \n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from time import monotonic\n",
        "\n",
        "\n",
        "import textwrap\n",
        "import os\n",
        "import ast\n",
        "from datasets import Dataset\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    answer_correctness,\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    answer_similarity\n",
        ")\n",
        "\n",
        "from helper_functions import num_tokens_from_string, replace_t_with_space, replace_double_lines_with_one_line, split_into_chapters, is_similarity_ratio_lower_than_th, analyse_metric_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting Preferred Encoding for PyPDF on Google Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6QeKRm37HwC"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding # For using PyPDF on google colab "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting OpenAI API Key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WsKZUN02pAI",
        "outputId": "11e3ae7a-e2ed-4224-c1e6-17f4316dd6f4"
      },
      "outputs": [],
      "source": [
        "# Prompt the user for their OpenAI API key\n",
        "api_key = input(\"Please enter your OpenAI API key: \")\n",
        "# Set the API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "print(\"OPENAI_API_KEY has been set!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Path to Harry Potter PDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mUYHG_S6y22"
      },
      "outputs": [],
      "source": [
        "hp_pdf_path =\"Harry_Potter_Book_1_The_Sorcerers_Stone.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splitting the PDF into Chapters and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDHfDODdTIBY"
      },
      "outputs": [],
      "source": [
        "chapters = split_into_chapters(hp_pdf_path) \n",
        "chapters = replace_t_with_space(chapters)\n",
        "print(len(chapters))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Prompt Template for Summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49RqsAhDxjFg"
      },
      "outputs": [],
      "source": [
        "summarization_prompt_template = \"\"\"Write an extensive summary of about of the following:\n",
        "\n",
        "{text}\n",
        "\n",
        "SUMMARY:\"\"\"\n",
        "\n",
        "summarization_prompt = PromptTemplate(template=summarization_prompt_template, input_variables=[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Function to Create Chapter Summaries using LLMs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehe6iObnx4l9"
      },
      "outputs": [],
      "source": [
        "def create_chapter_summary(chapter):\n",
        "    \"\"\"\n",
        "    Creates a summary of a chapter using a large language model (LLM).\n",
        "\n",
        "    Args:\n",
        "        chapter: A Document object representing the chapter to summarize.\n",
        "\n",
        "    Returns:\n",
        "        A Document object containing the summary of the chapter.\n",
        "    \"\"\"\n",
        "\n",
        "    chapter_txt = chapter.page_content  # Extract chapter text\n",
        "    model_name = \"gpt-3.5-turbo-0125\"  # Specify LLM model\n",
        "    llm = ChatOpenAI(temperature=0, model_name=model_name)  # Create LLM instance\n",
        "    gpt_35_turbo_max_tokens = 16000  # Maximum token limit for the LLM\n",
        "    verbose = False  # Set to True for more detailed output\n",
        "\n",
        "    # Calculate number of tokens in the chapter text\n",
        "    num_tokens = num_tokens_from_string(chapter_txt, model_name)\n",
        "\n",
        "    # Choose appropriate chain type based on token count\n",
        "    if num_tokens < gpt_35_turbo_max_tokens:\n",
        "        chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=summarization_prompt, verbose=verbose)\n",
        "    else:\n",
        "        chain = load_summarize_chain(llm, chain_type=\"map_reduce\", map_prompt=summarization_prompt, combine_prompt=summarization_prompt, verbose=verbose)\n",
        "\n",
        "    start_time = monotonic()  # Start timer\n",
        "    doc_chapter = Document(page_content=chapter_txt)  # Create Document object for chapter\n",
        "    summary = chain.invoke([doc_chapter])  # Generate summary using the chain\n",
        "    print(f\"Chain type: {chain.__class__.__name__}\")  # Print chain type\n",
        "    print(f\"Run time: {monotonic() - start_time}\")  # Print execution time\n",
        "\n",
        "    # Clean up summary text\n",
        "    summary = replace_double_lines_with_one_line(summary[\"output_text\"])\n",
        "\n",
        "    # Create Document object for summary\n",
        "    doc_summary = Document(page_content=summary, metadata=chapter.metadata)\n",
        "\n",
        "    return doc_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Summaries for Each Chapter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4UBurLsMCHj",
        "outputId": "668932cf-02d8-446d-e0ed-3f85b30d3bba"
      },
      "outputs": [],
      "source": [
        "chapter_summaries = []\n",
        "for chapter in chapters:\n",
        "    chapter_summaries.append(create_chapter_summary(chapter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function to Encode a Book into a Vector Store using OpenAI Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t1Kq6fGS0EA"
      },
      "outputs": [],
      "source": [
        "def encode_book(path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        path: The path to the PDF file.\n",
        "        chunk_size: The desired size of each text chunk.\n",
        "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded book content.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load PDF documents\n",
        "    loader = PyPDFLoader(path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    cleaned_texts = replace_t_with_space(texts)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
        "\n",
        "    return vectorstore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding Chapter Summaries into Vector Store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHt9Y12gMp2z"
      },
      "outputs": [],
      "source": [
        "def encode_chapter_summaries(chapter_summaries):\n",
        "    \"\"\"\n",
        "    Encodes a list of chapter summaries into a vector store using OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        chapter_summaries: A list of Document objects representing the chapter summaries.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded chapter summaries.\n",
        "    \"\"\"\n",
        "\n",
        "    embeddings = OpenAIEmbeddings()  # Create OpenAI embeddings\n",
        "    chapter_summaries_vectorstore = FAISS.from_documents(chapter_summaries, embeddings)  # Create vector store\n",
        "    return chapter_summaries_vectorstore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Vector Stores and Retrievers for Book and Chapter Summaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bWP7iLNS6Ey"
      },
      "outputs": [],
      "source": [
        "# Encode the book and chapter summaries into vector stores\n",
        "chunks_vector_store = encode_book(hp_pdf_path, chunk_size=1000, chunk_overlap=200)\n",
        "chapter_summaries_vector_store = encode_chapter_summaries(chapter_summaries)\n",
        "\n",
        "# Create retrievers for both vector stores\n",
        "chunks_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
        "chapter_summaries_retriever = chapter_summaries_vector_store.as_retriever(search_kwargs={\"k\": 4})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Sub-Questions for Vector Store Queries from User Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpVkNzd1hs8v"
      },
      "outputs": [],
      "source": [
        "modifier_prompt_template = \"\"\"\n",
        "You are an expert on analyzing and understanding books based solely on their textual content. You have no prior knowledge about the specific book in the question.\n",
        "If you ever read the related book before, FORGET ANYTHING about it.\n",
        "You have access to two vector stores:\n",
        "\n",
        "• One containing all the book content divided into chunks of 1000 characters with 200 character overlap.\n",
        "• Another containing summaries of each chapter, approximately 250 tokens each.\n",
        "\n",
        "Given a user's question about the book, your task is to generate a list of no more than 3 sub-questions that can be used as queries to retrieve relevant information from the vector stores based on semantic similarity. These sub-questions should be designed to collectively cover all the information needed to answer the original question comprehensively, without relying on any prior knowledge of the book's plot, characters, or events.\n",
        "\n",
        "When generating the sub-questions, consider the following:\n",
        "\n",
        "\n",
        "1.No Pre-knowledge: you're unaware of the book's details like plot or characters. The sub-questions must not present any prior knowledge of the book.\n",
        "2.Directly Derived: Create sub-questions strictly from the user's question, aiming to retrieve book information without presupposing specific plot details or events.\n",
        "3.Break Down: Decompose the user's question into finer, detailed sub-questions using only the textual content provided.\n",
        "4.Key Concepts: Identify essential information needed from the original question and form targeted sub-questions to gather this data.\n",
        "5.Self-contained Queries: Each sub-question should stand alone for effective vector store querying through semantic similarity.\n",
        "6.Logical Sequence: Arrange sub-questions in a logical order that collectively provides a thorough answer.\n",
        "7.Efficiency: Ensure sub-questions are unique and focused to avoid redundant searches and streamline information retrieval.\n",
        "\n",
        "Output your response as a Python list, where each item is a self-contained sub-question that can be used as a standalone query for vector retrieval.\n",
        "\n",
        "User's question: {question}\n",
        "\"\"\"\n",
        "modifier_prompt = PromptTemplate(\n",
        "input_variables=[\"question\"],\n",
        "template=modifier_prompt_template,\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-1106-preview\", max_tokens=4000)\n",
        "\n",
        "question_modifier_llm_chain = LLMChain(llm=llm, prompt=modifier_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modifying and Cleaning User Questions for Enhanced Querying\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwSLdGdb1FiO"
      },
      "outputs": [],
      "source": [
        "def modify_question(question):\n",
        "    \"\"\"\n",
        "    Modifies a question using a question modifier LLM chain and cleans up the output.\n",
        "\n",
        "    Args:\n",
        "        question: The input question string.\n",
        "\n",
        "    Returns:\n",
        "        A list of modified questions, or an empty list if there's an error.\n",
        "    \"\"\"\n",
        "\n",
        "    # Invoke the question modifier LLM chain\n",
        "    result = question_modifier_llm_chain.invoke(input=question)\n",
        "    modified_questions_str = result['text']\n",
        "\n",
        "    # Clean up the output string\n",
        "    clean_str = modified_questions_str.replace(\"```python\", \"\").replace(\"```\", \"\").strip()  # Remove Markdown code block syntax\n",
        "    clean_str = clean_str.replace(\"sub_questions = \", \"\").strip()  # Remove variable assignment\n",
        "    clean_str = textwrap.dedent(clean_str)  # Dedent to remove unexpected indents\n",
        "\n",
        "    # Attempt to convert the cleaned string into a list of questions\n",
        "    try:\n",
        "        modified_questions = ast.literal_eval(clean_str)\n",
        "    except SyntaxError as e:\n",
        "        print(f\"Syntax error during ast.literal_eval: {e}\")\n",
        "        modified_questions = []  # Default to an empty list in case of error\n",
        "\n",
        "    return modified_questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Generating Modified Questions from an Original Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXfrK0m5DUq8"
      },
      "outputs": [],
      "source": [
        "question = \"how did harry beat quirrell?\"\n",
        "modified_question = modify_question(question)\n",
        "print(modified_question) # watch new questions generated based on the original question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Context for a Question by Retrieving Relevant Documents and Summaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEFLePy-0YS3"
      },
      "outputs": [],
      "source": [
        "def create_context_per_question(question, multi_query_retriever, multi_query_retriever_chapter_summaries):\n",
        "    \"\"\"\n",
        "    Creates context for a question by retrieving relevant documents and chapter summaries.\n",
        "\n",
        "    Args:\n",
        "        question: The input question string.\n",
        "        multi_query_retriever: A retriever for retrieving relevant documents.\n",
        "        multi_query_retriever_chapter_summaries: A retriever for retrieving relevant chapter summaries.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing two strings:\n",
        "            - context: The concatenated content of relevant documents.\n",
        "            - context_summaries: The concatenated content of relevant chapter summaries with citation information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve relevant documents and chapter summaries\n",
        "    docs = multi_query_retriever.get_relevant_documents(question)\n",
        "    docs_summaries = multi_query_retriever_chapter_summaries.get_relevant_documents(question)\n",
        "\n",
        "    # Concatenate document content\n",
        "    context = \" \".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # Concatenate chapter summaries with citation information\n",
        "    context_summaries = \"\".join(\n",
        "        f\"{doc.page_content} (Chapter {doc.metadata['chapter']})\" for doc in docs_summaries\n",
        "    )\n",
        "\n",
        "    return context, context_summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comprehensive Question Answering Pipeline Using Context Retrieval and LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjk7u5ME7ZzX"
      },
      "outputs": [],
      "source": [
        "def answer_question_pipeline(question, chunks_retriever, chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm, similarity_th=0.5):\n",
        "    \"\"\"\n",
        "    Answers a question by retrieving relevant context, modifying the question, and using an LLM chain.\n",
        "\n",
        "    Args:\n",
        "        question: The input question string.\n",
        "        retriever: A retriever for retrieving relevant documents.\n",
        "        chapter_summaries_retriever: A retriever for retrieving relevant chapter summaries.\n",
        "        answer_from_context_llm_chain: An LLM chain for answering questions based on context.\n",
        "        multi_query_retriver_llm: An LLM for use in the MultiQueryRetriever.\n",
        "        similarity_th:  Similarity threshold for context accumulation\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            - result: The result of invoking the answer_from_context_llm_chain.\n",
        "            - all_context_book: The concatenated content of relevant documents.\n",
        "            - all_context_summaries: The concatenated content of relevant chapter summaries.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create MultiQueryRetrievers\n",
        "    multi_query_retriever = MultiQueryRetriever.from_llm(retriever=chunks_retriever, llm=multi_query_retriver_llm)\n",
        "    multi_query_retriever_chapter_summaries = MultiQueryRetriever.from_llm(retriever=chapter_summaries_retriever, llm=multi_query_retriver_llm)\n",
        "\n",
        "    # Modify the question\n",
        "    modified_questions = modify_question(question)\n",
        "\n",
        "    # Accumulate relevant context\n",
        "    all_context_book = \"\"\n",
        "    all_context_summaries = \"\"\n",
        "\n",
        "    for modified_question in modified_questions:\n",
        "        curr_question_relevant_context, curr_question_relevant_summaries_context = create_context_per_question(\n",
        "            modified_question, multi_query_retriever, multi_query_retriever_chapter_summaries\n",
        "        )\n",
        "\n",
        "        # Add context if it's not too similar to existing context\n",
        "        if is_similarity_ratio_lower_than_th(all_context_book, curr_question_relevant_context, similarity_th):\n",
        "            all_context_book += curr_question_relevant_context\n",
        "\n",
        "        if is_similarity_ratio_lower_than_th(all_context_summaries, curr_question_relevant_summaries_context, similarity_th):\n",
        "            all_context_summaries += curr_question_relevant_summaries_context\n",
        "\n",
        "    # Combine context from book and summaries\n",
        "    all_context = all_context_book + all_context_summaries\n",
        "\n",
        "    # Prepare input data for the LLM chain\n",
        "    input_data = {\n",
        "        \"context\": all_context,\n",
        "        \"question\": question\n",
        "    }\n",
        "\n",
        "    # Execute the LLM chain and get the response\n",
        "    result = answer_from_context_llm_chain.invoke(input=input_data)\n",
        "\n",
        "    return result, all_context_book, all_context_summaries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Template for Answering Questions Using Context-Specific Information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV6v5-ajP_i-"
      },
      "outputs": [],
      "source": [
        "answer_question_prompt_template = \"\"\"\n",
        "Based solely on the information provided in this context, and without using any information outside of this context, please answer the following question as concisely and as shortly as possible. You can rephrase the question for better fitting to the context.\n",
        "\n",
        "Context:{context}\n",
        "Question:{question}\n",
        "\n",
        "**If the answer cannot be derived from the context, or if it requires knowledge from outside sources, simply answer: \"I don't know\".**\n",
        "\n",
        "Please cite specific parts of the context in your answer to demonstrate how it supports your response.\n",
        "If the chapter number of the relevant context appears, specify it in your answer.\n",
        "\"\"\"\n",
        "answer_question_prompt = PromptTemplate(\n",
        "input_variables=[\"context\", \"question\"],\n",
        "template=answer_question_prompt_template,\n",
        ")\n",
        "\n",
        "multi_query_retriver_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-1106-preview\", max_tokens=4000)\n",
        "\n",
        "answer_from_context_llm_chain = LLMChain(llm=llm, prompt=answer_question_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example Execution of the Question Answering Pipeline for \"How Did Harry Beat Quirrell?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YlJOyrULV5c"
      },
      "outputs": [],
      "source": [
        "question = 'how did harry beat quirrell?'\n",
        "result, all_context_book, all_context_summaries = answer_question_pipeline(question,chunks_retriever,chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm)\n",
        "\n",
        "wrapped_all_context_book = textwrap.fill(all_context_book, width=120)\n",
        "print(f' conetxt book: {wrapped_all_context_book} \\n')\n",
        "\n",
        "wrapped_all_context_summaries = textwrap.fill(all_context_summaries, width=120)\n",
        "print(f' context summaries: {wrapped_all_context_summaries} \\n')\n",
        "\n",
        "wrapped_result = textwrap.fill(result['text'], width=120)\n",
        "print(f' answer: {wrapped_result}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBKQkR-W4ZyV"
      },
      "source": [
        "### Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME52zvjPNv47"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"Who gave Harry Potter his first broomstick?\",\n",
        "    \"What is the name of the three-headed dog guarding the Sorcerer's Stone?\",\n",
        "    \"Which house did the Sorting Hat initially consider for Harry?\",\n",
        "    \"What is the name of Harry's owl?\"\n",
        "]\n",
        "#     \"How did Harry and his friends get past Fluffy?\",\n",
        "#     \"What is the Mirror of Erised?\",\n",
        "#     \"Who tried to steal the Sorcerer's Stone?\",\n",
        "#     \"How did Harry defeat Quirrell/Voldemort?\",\n",
        "#     \"What is Harry's parent's secret weapon against Voldemort?\",\n",
        "# ]\n",
        "\n",
        "ground_truth_answers = [\n",
        "    \"Professor McGonagall\",\n",
        "    \"Fluffy\",\n",
        "    \"Slytherin\",\n",
        "    \"Hedwig\",\n",
        "    # \"They played music to put Fluffy to sleep.\",\n",
        "    # \"A magical mirror that shows the 'deepest, most desperate desire of our hearts.'\",\n",
        "    # \"Professor Quirrell, possessed by Voldemort\",\n",
        "    # \"Harry's mother's love protected him, causing Quirrell/Voldemort pain when they touched him.\",\n",
        "    # \"Love\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Answers and Retrieving Documents for Predefined Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "Wo9EEV0j4mJA",
        "outputId": "14c46214-8fb8-41aa-9845-73367d9a738f"
      },
      "outputs": [],
      "source": [
        "generated_answers = []\n",
        "retrieved_documents = []\n",
        "for question in questions:\n",
        "    result, all_context_book, all_context_summaries = answer_question_pipeline(question, chunks_retriever, chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm)\n",
        "    generated_answers.append(result['text'])\n",
        "    retrieved_documents.append(all_context_book + all_context_summaries)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Displaying Retrieved Documents and Generated Answers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'retrieved_documents: {retrieved_documents}\\n')\n",
        "print(f'generated_answers: {generated_answers}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing Data and Conducting Ragas Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for Ragas evaluation\n",
        "data_samples = {\n",
        "    'question': questions,  # Replace with your list of questions\n",
        "    'answer': generated_answers,  # Replace with your list of generated answers\n",
        "    'contexts': retrieved_documents,  # Your retrieved_documents list\n",
        "    'ground_truth': ground_truth_answers  # Replace with your list of ground truth answers\n",
        "}\n",
        "\n",
        "# Convert contexts to list of strings (if necessary)\n",
        "data_samples['contexts'] = [list(context) for context in data_samples['contexts']]\n",
        "\n",
        "dataset = Dataset.from_dict(data_samples)\n",
        "\n",
        "# Evaluate using Ragas with the specified metrics\n",
        "metrics = [\n",
        "    answer_correctness,\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    answer_similarity\n",
        "]\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-1106-preview\", max_tokens=4000)\n",
        "score = evaluate(dataset, metrics=metrics, llm=llm)\n",
        "\n",
        "# Print results and explanations\n",
        "results_df = score.to_pandas()\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyzing Metric Results from Ragas Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyse_metric_results(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interactive Chat Interface for Harry Potter Inquiries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV7UvEofS1nv"
      },
      "outputs": [],
      "source": [
        "def chat_with_data(chunks_retriever, chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm):\n",
        "    \"\"\"\n",
        "    Provides an interactive chat interface for answering questions about Harry Potter.\n",
        "\n",
        "    Args:\n",
        "        retriever: A retriever for retrieving relevant documents.\n",
        "        chapter_summaries_retriever: A retriever for retrieving relevant chapter summaries.\n",
        "        answer_from_context_llm_chain: An LLM chain for answering questions based on context.\n",
        "        multi_query_retriver_llm: An LLM for use in the MultiQueryRetriever.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"You can start chatting with me about Harry Potter. Type 'exit' to stop.\")\n",
        "\n",
        "    while True:\n",
        "        # Prompt the user for a question\n",
        "        question = input(\"What's your question? \\n\")\n",
        "\n",
        "        # Check if the user wants to exit\n",
        "        if question.lower() == 'exit':\n",
        "            print(\"Exiting chat. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Answer the question using the pipeline\n",
        "        result, _, _ = answer_question_pipeline(\n",
        "            question, chunks_retriever, chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm\n",
        "        )\n",
        "\n",
        "        # Print the answer\n",
        "        print(\"Answer:\")\n",
        "        wrapped_result = textwrap.fill(result['text'], width=120)  # Wrap text for readability\n",
        "        print(wrapped_result)\n",
        "        print(\"-\" * 80)  # Print a separator line for readability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calling the chat_with_data function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "048b2Ol7QAiF",
        "outputId": "1c891083-5c91-4425-ba23-50d1f32e0ac5"
      },
      "outputs": [],
      "source": [
        "chat_with_data(chunks_retriever,chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
