{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installing Libraries for LangChain and Supporting Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjDmS3Cn2jKm",
        "outputId": "569cfc27-0dbb-455a-85cb-3a75b57bbbf2"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install faiss-gpu\n",
        "!pip install langchain_experimental\n",
        "!pip install \"langchain[docarray]\"\n",
        "!pip install pylcs\n",
        "!pip3 install pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing Necessary Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "XIL5OCqJ7bh8"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import  PyPDFLoader\n",
        "from langchain.vectorstores import  FAISS\n",
        "from langchain.text_splitter import  RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings \n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from time import monotonic\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import textwrap\n",
        "import os\n",
        "import ast\n",
        "from datasets import Dataset\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    answer_correctness,\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    answer_similarity\n",
        ")\n",
        "\n",
        "from helper_functions import num_tokens_from_string, replace_t_with_space, replace_double_lines_with_one_line, split_into_chapters, is_similarity_ratio_lower_than_th, analyse_metric_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting Preferred Encoding for PyPDF on Google Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6QeKRm37HwC"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding # For using PyPDF on google colab "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting OpenAI API Key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WsKZUN02pAI",
        "outputId": "11e3ae7a-e2ed-4224-c1e6-17f4316dd6f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY has been set!\n"
          ]
        }
      ],
      "source": [
        "# Prompt the user for their OpenAI API key\n",
        "api_key = input(\"Please enter your OpenAI API key: \")\n",
        "# Set the API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "print(\"OPENAI_API_KEY has been set!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Path to Harry Potter PDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8mUYHG_S6y22"
      },
      "outputs": [],
      "source": [
        "hp_pdf_path =\"Harry_Potter_Book_1_The_Sorcerers_Stone.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splitting the PDF into Chapters and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cDHfDODdTIBY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17\n"
          ]
        }
      ],
      "source": [
        "chapters = split_into_chapters(hp_pdf_path) \n",
        "chapters = replace_t_with_space(chapters)\n",
        "print(len(chapters))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Prompt Template for Summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "49RqsAhDxjFg"
      },
      "outputs": [],
      "source": [
        "summarization_prompt_template = \"\"\"Write an extensive summary of about of the following:\n",
        "\n",
        "{text}\n",
        "\n",
        "SUMMARY:\"\"\"\n",
        "\n",
        "summarization_prompt = PromptTemplate(template=summarization_prompt_template, input_variables=[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Function to Create Chapter Summaries using LLMs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ehe6iObnx4l9"
      },
      "outputs": [],
      "source": [
        "def create_chapter_summary(chapter):\n",
        "    \"\"\"\n",
        "    Creates a summary of a chapter using a large language model (LLM).\n",
        "\n",
        "    Args:\n",
        "        chapter: A Document object representing the chapter to summarize.\n",
        "\n",
        "    Returns:\n",
        "        A Document object containing the summary of the chapter.\n",
        "    \"\"\"\n",
        "\n",
        "    chapter_txt = chapter.page_content  # Extract chapter text\n",
        "    model_name = \"gpt-3.5-turbo-0125\"  # Specify LLM model\n",
        "    llm = ChatOpenAI(temperature=0, model_name=model_name)  # Create LLM instance\n",
        "    gpt_35_turbo_max_tokens = 16000  # Maximum token limit for the LLM\n",
        "    verbose = False  # Set to True for more detailed output\n",
        "\n",
        "    # Calculate number of tokens in the chapter text\n",
        "    num_tokens = num_tokens_from_string(chapter_txt, model_name)\n",
        "\n",
        "    # Choose appropriate chain type based on token count\n",
        "    if num_tokens < gpt_35_turbo_max_tokens:\n",
        "        chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=summarization_prompt, verbose=verbose)\n",
        "    else:\n",
        "        chain = load_summarize_chain(llm, chain_type=\"map_reduce\", map_prompt=summarization_prompt, combine_prompt=summarization_prompt, verbose=verbose)\n",
        "\n",
        "    start_time = monotonic()  # Start timer\n",
        "    doc_chapter = Document(page_content=chapter_txt)  # Create Document object for chapter\n",
        "    summary = chain.invoke([doc_chapter])  # Generate summary using the chain\n",
        "    print(f\"Chain type: {chain.__class__.__name__}\")  # Print chain type\n",
        "    print(f\"Run time: {monotonic() - start_time}\")  # Print execution time\n",
        "\n",
        "    # Clean up summary text\n",
        "    summary = replace_double_lines_with_one_line(summary[\"output_text\"])\n",
        "\n",
        "    # Create Document object for summary\n",
        "    doc_summary = Document(page_content=summary, metadata=chapter.metadata)\n",
        "\n",
        "    return doc_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Summaries for Each Chapter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4UBurLsMCHj",
        "outputId": "668932cf-02d8-446d-e0ed-3f85b30d3bba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\N7\\Anaconda3\\envs\\openai-env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chain type: StuffDocumentsChain\n",
            "Run time: 12.952999999979511\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 10.734999999869615\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 7.922000000020489\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 6.405999999959022\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 9.234000000171363\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 9.343999999808148\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 6.780999999959022\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 9.125\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 7.2350000001024455\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 9.546999999787658\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 7.765999999828637\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 6.7030000002123415\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 6.0\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 9.781999999890104\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 7.733999999938533\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 10.218000000109896\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 6.547000000020489\n"
          ]
        }
      ],
      "source": [
        "chapter_summaries = []\n",
        "for chapter in chapters:\n",
        "    chapter_summaries.append(create_chapter_summary(chapter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function to Encode a Book into a Vector Store using OpenAI Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9t1Kq6fGS0EA"
      },
      "outputs": [],
      "source": [
        "def encode_book(path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        path: The path to the PDF file.\n",
        "        chunk_size: The desired size of each text chunk.\n",
        "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded book content.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load PDF documents\n",
        "    loader = PyPDFLoader(path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    cleaned_texts = replace_t_with_space(texts)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
        "\n",
        "    return vectorstore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding Chapter Summaries into Vector Store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FHt9Y12gMp2z"
      },
      "outputs": [],
      "source": [
        "def encode_chapter_summaries(chapter_summaries):\n",
        "    \"\"\"\n",
        "    Encodes a list of chapter summaries into a vector store using OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        chapter_summaries: A list of Document objects representing the chapter summaries.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded chapter summaries.\n",
        "    \"\"\"\n",
        "\n",
        "    embeddings = OpenAIEmbeddings()  # Create OpenAI embeddings\n",
        "    chapter_summaries_vectorstore = FAISS.from_documents(chapter_summaries, embeddings)  # Create vector store\n",
        "    return chapter_summaries_vectorstore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Vector Stores and Retrievers for Book and Chapter Summaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "4bWP7iLNS6Ey"
      },
      "outputs": [],
      "source": [
        "# Encode the book and chapter summaries into vector stores\n",
        "chunks_vector_store = encode_book(hp_pdf_path, chunk_size=1000, chunk_overlap=200)\n",
        "chapter_summaries_vector_store = encode_chapter_summaries(chapter_summaries)\n",
        "\n",
        "chunks_vector_store.save_local(\"chunks_vector_store\") # save the chunks_vector_store\n",
        "chapter_summaries_vector_store.save_local(\"chapter_summaries_vector_store\") # save the chapter_summaries_vector_store\n",
        "\n",
        "# Create retrievers for both vector stores\n",
        "chunks_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
        "chapter_summaries_retriever = chapter_summaries_vector_store.as_retriever(search_kwargs={\"k\": 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GROQ_API_KEY has been set!\n"
          ]
        }
      ],
      "source": [
        "from groq import Groq\n",
        "\n",
        "# Prompt the user for their OpenAI API key\n",
        "groq_api_key = input(\"Please enter your groq API key: \")\n",
        "\n",
        "groq = Groq(\n",
        "    api_key=groq_api_key,\n",
        ")\n",
        "# Set the API key as an environment variable\n",
        "print(\"GROQ_API_KEY has been set!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_chunks_context_per_question(question, chunks_query_retriever):\n",
        " \n",
        "    # Retrieve relevant documents\n",
        "    docs = chunks_query_retriever.get_relevant_documents(question)\n",
        "\n",
        "    # Concatenate document content\n",
        "    context = \" \".join(doc.page_content for doc in docs)\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "\n",
        "def create_summaries_context_per_question(question, chapter_summaries_query_retriever):\n",
        "   \n",
        "    # Retrieve relevant chapter summaries\n",
        "    docs_summaries = chapter_summaries_query_retriever.get_relevant_documents(question)\n",
        "\n",
        "\n",
        "    # Concatenate chapter summaries with citation information\n",
        "    context_summaries = \"\".join(\n",
        "        f\"{doc.page_content} (Chapter {doc.metadata['chapter']})\" for doc in docs_summaries\n",
        "    )\n",
        "\n",
        "    return context_summaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "keep_only_relevant_content_prompt_template = \"\"\"you receive a query: {query} and retrieved docuemnts: {retrieved_documents} from a vector store.\n",
        " You need to filter the retrieved data and keep only the sentences that are relevant, but all of them.\n",
        " you should output the distilled content in a json format. \n",
        " REMEMBER: the output has to be a json containing ALL the relevant sentences, and not the answer to the query. {format_instructions}\"\"\"\n",
        "\n",
        "class QuestionAnswer(BaseModel):\n",
        "    relevant_content: str = Field(description=\"The relevant content from the retrieved documents that is relevant to the query.\")\n",
        "\n",
        "\n",
        "json_parser = JsonOutputParser(pydantic_object=QuestionAnswer)\n",
        "\n",
        "keep_only_relevant_content_prompt = PromptTemplate(\n",
        "    template=keep_only_relevant_content_prompt_template,\n",
        "    input_variables=[\"query\", \"retrieved_documents\"],\n",
        "    partial_variables={\"format_instructions\": json_parser.get_format_instructions()}, \n",
        ")\n",
        "\n",
        "keep_only_relevant_content_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
        "keep_only_relevant_content_chain = keep_only_relevant_content_prompt | keep_only_relevant_content_llm | json_parser\n",
        "\n",
        "def keep_only_relevant_content(question, context, chain):\n",
        "    \"\"\"\n",
        "    Keeps only the relevant content from the retrieved documents that is relevant to the query.\n",
        "\n",
        "    Args:\n",
        "        question: The query question.\n",
        "        context: The retrieved documents.\n",
        "        chain: The LLMChain instance.\n",
        "\n",
        "    Returns:\n",
        "        The relevant content from the retrieved documents that is relevant to the query.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create Document objects for the query and retrieved documents\n",
        "    doc_query = Document(page_content=question)\n",
        "    doc_retrieved_documents = Document(page_content=context)\n",
        "\n",
        "    input_data = {\n",
        "    \"query\": doc_query,\n",
        "    \"retrieved_documents\": doc_retrieved_documents\n",
        "}\n",
        "    # Invoke the chain to keep only the relevant content\n",
        "    output = chain.invoke(input_data)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "can_be_answered_prompt_template = \"\"\"You receive a query: {question} and a context: {context}. \n",
        "You need to determine if the question can be answered based on the context.\n",
        "{format_instructions}\n",
        "\n",
        "**Answer:**\n",
        "\"\"\"\n",
        "\n",
        "class QuestionAnswer(BaseModel):\n",
        "    can_be_answered: bool = Field(description=\"binary result of whether the question can be answered or not\")\n",
        "\n",
        "json_parser = JsonOutputParser(pydantic_object=QuestionAnswer)\n",
        "\n",
        "answer_question_prompt = PromptTemplate(\n",
        "    template=can_be_answered_prompt_template,\n",
        "    input_variables=[\"question\",\"context\"],\n",
        "    partial_variables={\"format_instructions\": json_parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "can_be_answered_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
        "can_be_answered_chain = answer_question_prompt | can_be_answered_llm | json_parser\n",
        "\n",
        "\n",
        "def can_be_answered_from_contenxt(question, context, can_be_answered_chain):\n",
        "    \"\"\"\n",
        "    Determines if a question can be answered based on the context.\n",
        "\n",
        "    Args:\n",
        "        question: The query question.\n",
        "        context: The retrieved documents.\n",
        "        chain: The LLMChain instance.\n",
        "\n",
        "    Returns:\n",
        "        A binary result of whether the question can be answered or not.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create Document objects for the query and context\n",
        "    doc_question = Document(page_content=question)\n",
        "    doc_context = Document(page_content=context)\n",
        "\n",
        "    input_data = {\n",
        "        \"question\": doc_question,\n",
        "        \"context\": doc_context\n",
        "    }\n",
        "\n",
        "    # Invoke the chain to determine if the question can be answered\n",
        "    output = can_be_answered_chain.invoke(input_data)\n",
        "\n",
        "    return output\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_relevant_chunks_per_question(question, chunks_retriever, keep_only_relevant_content_chain):\n",
        "    \"\"\"\n",
        "    Retrieves relevant chunks of text from the book based on a question.\n",
        "\n",
        "    Args:\n",
        "        question: The question to ask.\n",
        "        chunks_retriever: The retriever for the book chunks.\n",
        "        keep_only_relevant_content_chain: The chain to keep only the relevant content.\n",
        "\n",
        "    Returns:\n",
        "        The relevant chunks of text from the book based on the question.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the context for the question\n",
        "    context = create_chunks_context_per_question(question, chunks_retriever)\n",
        "\n",
        "    # Keep only the relevant content from the retrieved documents that is relevant to the query\n",
        "    relevant_content = keep_only_relevant_content(question, context, keep_only_relevant_content_chain)\n",
        "\n",
        "    return relevant_content\n",
        "\n",
        "\n",
        "def get_relevant_summaries_per_question(question, chapter_summaries_retriever, keep_only_relevant_content_chain):\n",
        "    \"\"\"\n",
        "    Retrieves relevant chapter summaries based on a question.\n",
        "\n",
        "    Args:\n",
        "        question: The question to ask.\n",
        "        chapter_summaries_retriever: The retriever for the chapter summaries.\n",
        "        keep_only_relevant_content_chain: The chain to keep only the relevant content.\n",
        "\n",
        "    Returns:\n",
        "        The relevant chapter summaries based on the question.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the context for the question\n",
        "    context_summaries = create_summaries_context_per_question(question, chapter_summaries_retriever)\n",
        "\n",
        "    # Keep only the relevant content from the retrieved documents that is relevant to the query\n",
        "    relevant_content_summaries = keep_only_relevant_content(question, context_summaries, keep_only_relevant_content_chain)\n",
        "\n",
        "    return relevant_content_summaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.memory import ConversationBufferWindowMemory, ConversationSummaryMemory\n",
        "\n",
        "\n",
        "llm_retrieval_agent = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
        "\n",
        "\n",
        "# # Craft agent description emphasizing reliance on retrieval\n",
        "agent_description = \"\"\"\n",
        "You are an AI assistant tasked with answering questions about the content of a book which is encoded into vector stores. you have both chunks encoded and chapter summaries encoded.\n",
        "You have no prior knowledge of any book, including characters, places, or events. \n",
        "Your answers must be based solely on the information you retrieve using the provided tools.\n",
        "You can use the RetrieveBookContent tool to search for specific details within the book content, and the Retrievechaptersummary tool to get a high-level overview of events and key points from the chapter summaries.\n",
        "every answer you provide should be supported by a *CITED* evidence from the book content or chapter summaries.\n",
        "if no relevant information is found, you should indicate that you were unable to find any relevant information.\n",
        "you must also determine if a question can be answered based on the context provided.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "tools = [\n",
        "Tool(\n",
        "    name=\"RetrieveBookContent\",\n",
        "    func=lambda question: get_relevant_chunks_per_question(question, chunks_retriever, keep_only_relevant_content_chain),\n",
        "    description=\"Retrieves relevant chunks of text from the book based on a question.\",\n",
        "),\n",
        "Tool(\n",
        "    name=\"RetrieveChapterSummary\",\n",
        "    func=lambda question: get_relevant_summaries_per_question(question, chapter_summaries_retriever, keep_only_relevant_content_chain),\n",
        "    description=\"Retrieves relevant chapter summaries based on a question.\",\n",
        "),\n",
        "Tool(\n",
        "    name=\"CanBeAnswered\",\n",
        "    func=lambda question, context: can_be_answered_from_contenxt(question, context, can_be_answered_chain),\n",
        "    description=\"Determines if a question can be answered based on the context.\",\n",
        "),\n",
        "]\n",
        "\n",
        "\n",
        "# Initialize QA Chain and Memory\n",
        "chain = load_qa_chain(llm_retrieval_agent, chain_type=\"stuff\", verbose=True)  # Properly initialize the QA chain\n",
        "# memory = ConversationBufferWindowMemory(k=2, return_messages=True)\n",
        "\n",
        "# Initialize Agent with QA Chain\n",
        "agent = initialize_agent(\n",
        "    tools, \n",
        "    llm_retrieval_agent, \n",
        "    agent=\"zero-shot-react-description\", \n",
        "    verbose=True,\n",
        "    agent_description=agent_description,\n",
        "    # max_iterations=7,\n",
        "    # memory=memory,\n",
        "    handle_parsing_errors=True,\n",
        "    chain=chain,  # Include the QA chain in agent initialization\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to clarify the question, is it \"Charles Darwin\" instead of \"charls darvin\"?\n",
            "\n",
            "Action: CanBeAnswered\n",
            "Action Input: \"who is charls darvin?\"\u001b[0m"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "<lambda>() missing 1 required positional argument: 'context'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[116], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwho is charls darvin?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\N7\\Anaconda3\\envs\\openai-env\\Lib\\site-packages\\langchain\\chains\\base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
            "File \u001b[1;32mc:\\Users\\N7\\Anaconda3\\envs\\openai-env\\Lib\\site-packages\\langchain\\chains\\base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    160\u001b[0m     )\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\N7\\Anaconda3\\envs\\openai-env\\Lib\\site-packages\\langchain\\agents\\agent.py:1432\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1432\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1438\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1440\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1441\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1442\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\N7\\Anaconda3\\envs\\openai-env\\Lib\\site-packages\\langchain\\agents\\agent.py:1138\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1131\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1136\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1138\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1148\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\N7\\Anaconda3\\envs\\openai-env\\Lib\\site-packages\\langchain\\agents\\agent.py:1223\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1221\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m agent_action\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[1;32m-> 1223\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_agent_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\N7\\Anaconda3\\envs\\openai-env\\Lib\\site-packages\\langchain\\agents\\agent.py:1245\u001b[0m, in \u001b[0;36mAgentExecutor._perform_agent_action\u001b[1;34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[0m\n\u001b[0;32m   1243\u001b[0m         tool_run_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[1;32m-> 1245\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1253\u001b[0m     tool_run_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mtool_run_logging_kwargs()\n",
            "File \u001b[1;32mc:\\Users\\N7\\Anaconda3\\envs\\openai-env\\Lib\\site-packages\\langchain_core\\tools.py:452\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, **kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[1;32m--> 452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    454\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(observation, color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\N7\\Anaconda3\\envs\\openai-env\\Lib\\site-packages\\langchain_core\\tools.py:409\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, **kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m     parsed_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_input(tool_input)\n\u001b[0;32m    407\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[0;32m    408\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 409\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run, \u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[0;32m    414\u001b[0m     )\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_validation_error:\n",
            "File \u001b[1;32mc:\\Users\\N7\\Anaconda3\\envs\\openai-env\\Lib\\site-packages\\langchain_core\\tools.py:633\u001b[0m, in \u001b[0;36mTool._run\u001b[1;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc:\n\u001b[0;32m    625\u001b[0m     new_argument_supported \u001b[38;5;241m=\u001b[39m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    627\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\n\u001b[0;32m    628\u001b[0m             \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    629\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    630\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    631\u001b[0m         )\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_argument_supported\n\u001b[1;32m--> 633\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m     )\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool does not support sync\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: <lambda>() missing 1 required positional argument: 'context'"
          ]
        }
      ],
      "source": [
        "query = \"who is charls darvin?\"\n",
        "result = agent.invoke(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find information about Darvin in the book content.\n",
            "\n",
            "Action: RetrieveBookContent\n",
            "Action Input: \"Darvin\"\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m[(Document(page_content='“P-P-Petunia!” he gasped.\\n      Dudley tried to grab the letter to read it, but Uncle Vernon held it high\\nout of his reach. Aunt Petunia took it curiously and read the first line. For a\\nmoment it looked as though she might faint. She clutched her throat and made a\\nchoking noise.\\n      “Vernon! Oh my goodness — Vernon!”\\n      They stared at each other, seeming to have forgotten that Harry and\\nDudley were still in the room. Dudley wasn’t used to being ignored. He gave his\\nfather a sharp tap on the head with his Smelting stick.', metadata={'source': 'Harry_Potter_Book_1_The_Sorcerers_Stone.pdf', 'page': 27}), 0.42526704), (Document(page_content='lot like yer dad, but yeh’ve got yer mom’s eyes.”\\n      Uncle Vernon made a funny rasping noise.\\n      “I demand that you leave at once, sir!” he said. “You are breaking and\\nentering!”\\n      “Ah, shut up, Dursley, yeh great prune,” said the giant; he reached over', metadata={'source': 'Harry_Potter_Book_1_The_Sorcerers_Stone.pdf', 'page': 36}), 0.43815932), (Document(page_content='Mr. H. Potter\\nRoom 17\\nRailview Hotel\\nCokeworth\\n \\nHarry made a grab for the letter but Uncle Vernon knocked his hand out of\\nthe way. The woman stared.\\n      “I’ll take them,” said Uncle Vernon, standing up quickly and following\\nher from the dining room.\\n* * *\\n“Wouldn’t it be better just to go home, dear?” Aunt Petunia suggested\\ntimidly, hours later, but Uncle Vernon didn’t seem to hear her. Exactly what he\\nwas looking for, none of them knew. He drove them into the middle of a forest,\\ngot out, looked around, shook his head, got back in the car, and off they went\\nagain. The same thing happened in the middle of a plowed field, halfway across\\na suspension bridge, and at the top of a multilevel parking garage.\\n      “Daddy’s gone mad, hasn’t he?” Dudley asked Aunt Petunia dully late\\nthat afternoon. Uncle Vernon had parked at the coast, locked them all inside the\\ncar, and disappeared.\\n      It started to rain. Great drops beat on the roof of the car. Dudley sniveled.', metadata={'source': 'Harry_Potter_Book_1_The_Sorcerers_Stone.pdf', 'page': 32}), 0.44715178), (Document(page_content='“Darling, you haven’t counted Auntie Marge’s present, see, it’s here\\nunder this big one from Mummy and Daddy.”\\n      “All right, thirty-seven then,” said Dudley, going red in the face. Harry,\\nwho could see a huge Dudley tantrum coming on, began wolfing down his bacon\\nas fast as possible in case Dudley turned the table over.\\n      Aunt Petunia obviously scented danger, too, because she said quickly,\\n“And we’ll buy you another two presents while we’re out today. How’s that,\\npopkin? Two more presents. Is that all right”\\n      Dudley thought for a moment. It looked like hard work. Finally he said\\nslowly, “So I’ll have thirty...thirty...”\\n      “Thirty-nine, sweetums,” said Aunt Petunia.\\n      “Oh.” Dudley sat down heavily and grabbed the nearest parcel. “All right\\nthen.”\\n      Uncle Vernon chuckled.\\n      “Little tyke wants his money’s worth, just like his father. ’Atta boy,\\nDudley!” He ruffled Dudley’s hair.\\n      At that moment the telephone rang and Aunt Petunia went to answer it', metadata={'source': 'Harry_Potter_Book_1_The_Sorcerers_Stone.pdf', 'page': 18}), 0.45046237)]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: It seems like I have multiple results related to Darvin, but none of them seem to be a person. I need to investigate further.\n",
            "\n",
            "Action: RetrieveBookContent\n",
            "Action Input: \"Darvin\" (again, to see if I can find more context)\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m[(Document(page_content='Mr. H. Potter\\nRoom 17\\nRailview Hotel\\nCokeworth\\n \\nHarry made a grab for the letter but Uncle Vernon knocked his hand out of\\nthe way. The woman stared.\\n      “I’ll take them,” said Uncle Vernon, standing up quickly and following\\nher from the dining room.\\n* * *\\n“Wouldn’t it be better just to go home, dear?” Aunt Petunia suggested\\ntimidly, hours later, but Uncle Vernon didn’t seem to hear her. Exactly what he\\nwas looking for, none of them knew. He drove them into the middle of a forest,\\ngot out, looked around, shook his head, got back in the car, and off they went\\nagain. The same thing happened in the middle of a plowed field, halfway across\\na suspension bridge, and at the top of a multilevel parking garage.\\n      “Daddy’s gone mad, hasn’t he?” Dudley asked Aunt Petunia dully late\\nthat afternoon. Uncle Vernon had parked at the coast, locked them all inside the\\ncar, and disappeared.\\n      It started to rain. Great drops beat on the roof of the car. Dudley sniveled.', metadata={'source': 'Harry_Potter_Book_1_The_Sorcerers_Stone.pdf', 'page': 32}), 0.4367238), (Document(page_content='“P-P-Petunia!” he gasped.\\n      Dudley tried to grab the letter to read it, but Uncle Vernon held it high\\nout of his reach. Aunt Petunia took it curiously and read the first line. For a\\nmoment it looked as though she might faint. She clutched her throat and made a\\nchoking noise.\\n      “Vernon! Oh my goodness — Vernon!”\\n      They stared at each other, seeming to have forgotten that Harry and\\nDudley were still in the room. Dudley wasn’t used to being ignored. He gave his\\nfather a sharp tap on the head with his Smelting stick.', metadata={'source': 'Harry_Potter_Book_1_The_Sorcerers_Stone.pdf', 'page': 27}), 0.43828404), (Document(page_content='hands together. “And this gentleman’s kindly agreed to lend us his boat!”\\n      A toothless old man came ambling up to them, pointing, with a rather\\nwicked grin, at an old rowboat bobbing in the iron-gray water below them.\\n      “I’ve already got us some rations,” said Uncle Vernon, “so all aboard!”\\n      It was freezing in the boat. Icy sea spray and rain crept down their necks\\nand a chilly wind whipped their faces. After what seemed like hours they\\nreached the rock, where Uncle Vernon, slipping and sliding, led the way to the\\nbroken-down house.\\n      The inside was horrible; it smelled strongly of seaweed, the wind\\nwhistled through the gaps in the wooden walls, and the fireplace was damp and\\nempty. There were only two rooms.\\n      Uncle Vernon’s rations turned out to be a bag of chips each and four\\nbananas. He tried to start a fire but the empty chip bags just smoked and\\nshriveled up.\\n      “Could do with some of those letters now, eh?” he said cheerfully.', metadata={'source': 'Harry_Potter_Book_1_The_Sorcerers_Stone.pdf', 'page': 33}), 0.4400955), (Document(page_content='“Darling, you haven’t counted Auntie Marge’s present, see, it’s here\\nunder this big one from Mummy and Daddy.”\\n      “All right, thirty-seven then,” said Dudley, going red in the face. Harry,\\nwho could see a huge Dudley tantrum coming on, began wolfing down his bacon\\nas fast as possible in case Dudley turned the table over.\\n      Aunt Petunia obviously scented danger, too, because she said quickly,\\n“And we’ll buy you another two presents while we’re out today. How’s that,\\npopkin? Two more presents. Is that all right”\\n      Dudley thought for a moment. It looked like hard work. Finally he said\\nslowly, “So I’ll have thirty...thirty...”\\n      “Thirty-nine, sweetums,” said Aunt Petunia.\\n      “Oh.” Dudley sat down heavily and grabbed the nearest parcel. “All right\\nthen.”\\n      Uncle Vernon chuckled.\\n      “Little tyke wants his money’s worth, just like his father. ’Atta boy,\\nDudley!” He ruffled Dudley’s hair.\\n      At that moment the telephone rang and Aunt Petunia went to answer it', metadata={'source': 'Harry_Potter_Book_1_The_Sorcerers_Stone.pdf', 'page': 18}), 0.45178056)]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mIt seems like I've reached a dead end. I've searched for \"Darvin\" in the book content, but I couldn't find any relevant information about a person named Darvin. The results I got were mostly related to the Dursley family and their interactions with Harry Potter. I think it's safe to conclude that Darvin is not a character in the book.\n",
            "\n",
            "Final Answer: Darvin is not a character in the book.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'who is the protagonist in the book?', 'output': 'Harry Potter'}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Sub-Questions for Vector Store Queries from User Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpVkNzd1hs8v"
      },
      "outputs": [],
      "source": [
        "modifier_prompt_template = \"\"\"\n",
        "You are an expert on analyzing and understanding books based solely on their textual content. You have no prior knowledge about the specific book in the question.\n",
        "If you ever read the related book before, FORGET ANYTHING about it.\n",
        "You have access to two vector stores:\n",
        "\n",
        "• One containing all the book content divided into chunks of 1000 characters with 200 character overlap.\n",
        "• Another containing summaries of each chapter, approximately 250 tokens each.\n",
        "\n",
        "Given a user's question about the book, your task is to generate a list of no more than 3 sub-questions that can be used as queries to retrieve relevant information from the vector stores based on semantic similarity. These sub-questions should be designed to collectively cover all the information needed to answer the original question comprehensively, without relying on any prior knowledge of the book's plot, characters, or events.\n",
        "\n",
        "When generating the sub-questions, consider the following:\n",
        "\n",
        "\n",
        "1.No Pre-knowledge: you're unaware of the book's details like plot or characters. The sub-questions must not present any prior knowledge of the book.\n",
        "2.Directly Derived: Create sub-questions strictly from the user's question, aiming to retrieve book information without presupposing specific plot details or events.\n",
        "3.Break Down: Decompose the user's question into finer, detailed sub-questions using only the textual content provided.\n",
        "4.Key Concepts: Identify essential information needed from the original question and form targeted sub-questions to gather this data.\n",
        "5.Self-contained Queries: Each sub-question should stand alone for effective vector store querying through semantic similarity.\n",
        "6.Logical Sequence: Arrange sub-questions in a logical order that collectively provides a thorough answer.\n",
        "7.Efficiency: Ensure sub-questions are unique and focused to avoid redundant searches and streamline information retrieval.\n",
        "\n",
        "Output your response as a Python list, where each item is a self-contained sub-question that can be used as a standalone query for vector retrieval.\n",
        "\n",
        "User's question: {question}\n",
        "\"\"\"\n",
        "modifier_prompt = PromptTemplate(\n",
        "input_variables=[\"question\"],\n",
        "template=modifier_prompt_template,\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-1106-preview\", max_tokens=4000)\n",
        "\n",
        "question_modifier_llm_chain = LLMChain(llm=llm, prompt=modifier_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modifying and Cleaning User Questions for Enhanced Querying\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwSLdGdb1FiO"
      },
      "outputs": [],
      "source": [
        "def modify_question(question):\n",
        "    \"\"\"\n",
        "    Modifies a question using a question modifier LLM chain and cleans up the output.\n",
        "\n",
        "    Args:\n",
        "        question: The input question string.\n",
        "\n",
        "    Returns:\n",
        "        A list of modified questions, or an empty list if there's an error.\n",
        "    \"\"\"\n",
        "\n",
        "    # Invoke the question modifier LLM chain\n",
        "    result = question_modifier_llm_chain.invoke(input=question)\n",
        "    modified_questions_str = result['text']\n",
        "\n",
        "    # Clean up the output string\n",
        "    clean_str = modified_questions_str.replace(\"```python\", \"\").replace(\"```\", \"\").strip()  # Remove Markdown code block syntax\n",
        "    clean_str = clean_str.replace(\"sub_questions = \", \"\").strip()  # Remove variable assignment\n",
        "    clean_str = textwrap.dedent(clean_str)  # Dedent to remove unexpected indents\n",
        "\n",
        "    # Attempt to convert the cleaned string into a list of questions\n",
        "    try:\n",
        "        modified_questions = ast.literal_eval(clean_str)\n",
        "    except SyntaxError as e:\n",
        "        print(f\"Syntax error during ast.literal_eval: {e}\")\n",
        "        modified_questions = []  # Default to an empty list in case of error\n",
        "\n",
        "    return modified_questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Generating Modified Questions from an Original Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXfrK0m5DUq8"
      },
      "outputs": [],
      "source": [
        "question = \"how did harry beat quirrell?\"\n",
        "modified_question = modify_question(question)\n",
        "print(modified_question) # watch new questions generated based on the original question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Context for a Question by Retrieving Relevant Documents and Summaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEFLePy-0YS3"
      },
      "outputs": [],
      "source": [
        "def create_context_per_question(question, multi_query_retriever, multi_query_retriever_chapter_summaries):\n",
        "    \"\"\"\n",
        "    Creates context for a question by retrieving relevant documents and chapter summaries.\n",
        "\n",
        "    Args:\n",
        "        question: The input question string.\n",
        "        multi_query_retriever: A retriever for retrieving relevant documents.\n",
        "        multi_query_retriever_chapter_summaries: A retriever for retrieving relevant chapter summaries.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing two strings:\n",
        "            - context: The concatenated content of relevant documents.\n",
        "            - context_summaries: The concatenated content of relevant chapter summaries with citation information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve relevant documents and chapter summaries\n",
        "    docs = multi_query_retriever.get_relevant_documents(question)\n",
        "    docs_summaries = multi_query_retriever_chapter_summaries.get_relevant_documents(question)\n",
        "\n",
        "    # Concatenate document content\n",
        "    context = \" \".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # Concatenate chapter summaries with citation information\n",
        "    context_summaries = \"\".join(\n",
        "        f\"{doc.page_content} (Chapter {doc.metadata['chapter']})\" for doc in docs_summaries\n",
        "    )\n",
        "\n",
        "    return context, context_summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comprehensive Question Answering Pipeline Using Context Retrieval and LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjk7u5ME7ZzX"
      },
      "outputs": [],
      "source": [
        "def answer_question_pipeline(question, chunks_retriever, chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm, similarity_th=0.5):\n",
        "    \"\"\"\n",
        "    Answers a question by retrieving relevant context, modifying the question, and using an LLM chain.\n",
        "\n",
        "    Args:\n",
        "        question: The input question string.\n",
        "        retriever: A retriever for retrieving relevant documents.\n",
        "        chapter_summaries_retriever: A retriever for retrieving relevant chapter summaries.\n",
        "        answer_from_context_llm_chain: An LLM chain for answering questions based on context.\n",
        "        multi_query_retriver_llm: An LLM for use in the MultiQueryRetriever.\n",
        "        similarity_th:  Similarity threshold for context accumulation\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            - result: The result of invoking the answer_from_context_llm_chain.\n",
        "            - all_context_book: The concatenated content of relevant documents.\n",
        "            - all_context_summaries: The concatenated content of relevant chapter summaries.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create MultiQueryRetrievers\n",
        "    multi_query_retriever = MultiQueryRetriever.from_llm(retriever=chunks_retriever, llm=multi_query_retriver_llm)\n",
        "    multi_query_retriever_chapter_summaries = MultiQueryRetriever.from_llm(retriever=chapter_summaries_retriever, llm=multi_query_retriver_llm)\n",
        "\n",
        "    # Modify the question\n",
        "    modified_questions = modify_question(question)\n",
        "\n",
        "    # Accumulate relevant context\n",
        "    all_context_book = \"\"\n",
        "    all_context_summaries = \"\"\n",
        "\n",
        "    for modified_question in modified_questions:\n",
        "        curr_question_relevant_context, curr_question_relevant_summaries_context = create_context_per_question(\n",
        "            modified_question, multi_query_retriever, multi_query_retriever_chapter_summaries\n",
        "        )\n",
        "\n",
        "        # Add context if it's not too similar to existing context\n",
        "        if is_similarity_ratio_lower_than_th(all_context_book, curr_question_relevant_context, similarity_th):\n",
        "            all_context_book += curr_question_relevant_context\n",
        "\n",
        "        if is_similarity_ratio_lower_than_th(all_context_summaries, curr_question_relevant_summaries_context, similarity_th):\n",
        "            all_context_summaries += curr_question_relevant_summaries_context\n",
        "\n",
        "    # Combine context from book and summaries\n",
        "    all_context = all_context_book + all_context_summaries\n",
        "\n",
        "    # Prepare input data for the LLM chain\n",
        "    input_data = {\n",
        "        \"context\": all_context,\n",
        "        \"question\": question\n",
        "    }\n",
        "\n",
        "    # Execute the LLM chain and get the response\n",
        "    result = answer_from_context_llm_chain.invoke(input=input_data)\n",
        "\n",
        "    return result, all_context_book, all_context_summaries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Template for Answering Questions Using Context-Specific Information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV6v5-ajP_i-"
      },
      "outputs": [],
      "source": [
        "answer_question_prompt_template = \"\"\"\n",
        "Based solely on the information provided in this context, and without using any information outside of this context, please answer the following question as concisely and as shortly as possible. You can rephrase the question for better fitting to the context.\n",
        "\n",
        "Context:{context}\n",
        "Question:{question}\n",
        "\n",
        "**If the answer cannot be derived from the context, or if it requires knowledge from outside sources, simply answer: \"I don't know\".**\n",
        "\n",
        "Please cite specific parts of the context in your answer to demonstrate how it supports your response.\n",
        "If the chapter number of the relevant context appears, specify it in your answer.\n",
        "\"\"\"\n",
        "answer_question_prompt = PromptTemplate(\n",
        "input_variables=[\"context\", \"question\"],\n",
        "template=answer_question_prompt_template,\n",
        ")\n",
        "\n",
        "multi_query_retriver_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-1106-preview\", max_tokens=4000)\n",
        "\n",
        "answer_from_context_llm_chain = LLMChain(llm=llm, prompt=answer_question_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example Execution of the Question Answering Pipeline for \"How Did Harry Beat Quirrell?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YlJOyrULV5c"
      },
      "outputs": [],
      "source": [
        "question = 'how did harry beat quirrell?'\n",
        "result, all_context_book, all_context_summaries = answer_question_pipeline(question,chunks_retriever,chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm)\n",
        "\n",
        "wrapped_all_context_book = textwrap.fill(all_context_book, width=120)\n",
        "print(f' conetxt book: {wrapped_all_context_book} \\n')\n",
        "\n",
        "wrapped_all_context_summaries = textwrap.fill(all_context_summaries, width=120)\n",
        "print(f' context summaries: {wrapped_all_context_summaries} \\n')\n",
        "\n",
        "wrapped_result = textwrap.fill(result['text'], width=120)\n",
        "print(f' answer: {wrapped_result}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBKQkR-W4ZyV"
      },
      "source": [
        "### Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME52zvjPNv47"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"Who gave Harry Potter his first broomstick?\",\n",
        "    \"What is the name of the three-headed dog guarding the Sorcerer's Stone?\",\n",
        "    \"Which house did the Sorting Hat initially consider for Harry?\",\n",
        "    \"What is the name of Harry's owl?\"\n",
        "]\n",
        "#     \"How did Harry and his friends get past Fluffy?\",\n",
        "#     \"What is the Mirror of Erised?\",\n",
        "#     \"Who tried to steal the Sorcerer's Stone?\",\n",
        "#     \"How did Harry defeat Quirrell/Voldemort?\",\n",
        "#     \"What is Harry's parent's secret weapon against Voldemort?\",\n",
        "# ]\n",
        "\n",
        "ground_truth_answers = [\n",
        "    \"Professor McGonagall\",\n",
        "    \"Fluffy\",\n",
        "    \"Slytherin\",\n",
        "    \"Hedwig\",\n",
        "    # \"They played music to put Fluffy to sleep.\",\n",
        "    # \"A magical mirror that shows the 'deepest, most desperate desire of our hearts.'\",\n",
        "    # \"Professor Quirrell, possessed by Voldemort\",\n",
        "    # \"Harry's mother's love protected him, causing Quirrell/Voldemort pain when they touched him.\",\n",
        "    # \"Love\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Answers and Retrieving Documents for Predefined Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "Wo9EEV0j4mJA",
        "outputId": "14c46214-8fb8-41aa-9845-73367d9a738f"
      },
      "outputs": [],
      "source": [
        "generated_answers = []\n",
        "retrieved_documents = []\n",
        "for question in questions:\n",
        "    result, all_context_book, all_context_summaries = answer_question_pipeline(question, chunks_retriever, chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm)\n",
        "    generated_answers.append(result['text'])\n",
        "    retrieved_documents.append(all_context_book + all_context_summaries)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Displaying Retrieved Documents and Generated Answers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'retrieved_documents: {retrieved_documents}\\n')\n",
        "print(f'generated_answers: {generated_answers}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing Data and Conducting Ragas Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for Ragas evaluation\n",
        "data_samples = {\n",
        "    'question': questions,  # Replace with your list of questions\n",
        "    'answer': generated_answers,  # Replace with your list of generated answers\n",
        "    'contexts': retrieved_documents,  # Your retrieved_documents list\n",
        "    'ground_truth': ground_truth_answers  # Replace with your list of ground truth answers\n",
        "}\n",
        "\n",
        "# Convert contexts to list of strings (if necessary)\n",
        "data_samples['contexts'] = [list(context) for context in data_samples['contexts']]\n",
        "\n",
        "dataset = Dataset.from_dict(data_samples)\n",
        "\n",
        "# Evaluate using Ragas with the specified metrics\n",
        "metrics = [\n",
        "    answer_correctness,\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    answer_similarity\n",
        "]\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-1106-preview\", max_tokens=4000)\n",
        "score = evaluate(dataset, metrics=metrics, llm=llm)\n",
        "\n",
        "# Print results and explanations\n",
        "results_df = score.to_pandas()\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyzing Metric Results from Ragas Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyse_metric_results(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interactive Chat Interface for Harry Potter Inquiries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV7UvEofS1nv"
      },
      "outputs": [],
      "source": [
        "def chat_with_data(chunks_retriever, chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm):\n",
        "    \"\"\"\n",
        "    Provides an interactive chat interface for answering questions about Harry Potter.\n",
        "\n",
        "    Args:\n",
        "        retriever: A retriever for retrieving relevant documents.\n",
        "        chapter_summaries_retriever: A retriever for retrieving relevant chapter summaries.\n",
        "        answer_from_context_llm_chain: An LLM chain for answering questions based on context.\n",
        "        multi_query_retriver_llm: An LLM for use in the MultiQueryRetriever.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"You can start chatting with me about Harry Potter. Type 'exit' to stop.\")\n",
        "\n",
        "    while True:\n",
        "        # Prompt the user for a question\n",
        "        question = input(\"What's your question? \\n\")\n",
        "\n",
        "        # Check if the user wants to exit\n",
        "        if question.lower() == 'exit':\n",
        "            print(\"Exiting chat. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Answer the question using the pipeline\n",
        "        result, _, _ = answer_question_pipeline(\n",
        "            question, chunks_retriever, chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm\n",
        "        )\n",
        "\n",
        "        # Print the answer\n",
        "        print(\"Answer:\")\n",
        "        wrapped_result = textwrap.fill(result['text'], width=120)  # Wrap text for readability\n",
        "        print(wrapped_result)\n",
        "        print(\"-\" * 80)  # Print a separator line for readability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calling the chat_with_data function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "048b2Ol7QAiF",
        "outputId": "1c891083-5c91-4425-ba23-50d1f32e0ac5"
      },
      "outputs": [],
      "source": [
        "chat_with_data(chunks_retriever,chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
