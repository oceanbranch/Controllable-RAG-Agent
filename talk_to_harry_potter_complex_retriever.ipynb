{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing Necessary Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XIL5OCqJ7bh8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import  PyPDFLoader\n",
        "from langchain.vectorstores import  FAISS\n",
        "from langchain.text_splitter import  RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings \n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from time import monotonic\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from dotenv import load_dotenv\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from pprint import pprint\n",
        "import textwrap\n",
        "import os\n",
        "import ast\n",
        "from datasets import Dataset\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    answer_correctness,\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    answer_similarity\n",
        ")\n",
        "\n",
        "from helper_functions import num_tokens_from_string, replace_t_with_space, replace_double_lines_with_one_line, split_into_chapters,\\\n",
        "is_similarity_ratio_lower_than_th, analyse_metric_results, escape_quotes\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting Preferred Encoding for PyPDF on Google Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6QeKRm37HwC"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding # For using PyPDF on google colab "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting OPENAI and GROQ API keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "groq_api_key = os.getenv('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Path to Harry Potter PDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8mUYHG_S6y22"
      },
      "outputs": [],
      "source": [
        "hp_pdf_path =\"Harry_Potter_Book_1_The_Sorcerers_Stone.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splitting the PDF into Chapters and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cDHfDODdTIBY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17\n"
          ]
        }
      ],
      "source": [
        "chapters = split_into_chapters(hp_pdf_path) \n",
        "chapters = replace_t_with_space(chapters)\n",
        "print(len(chapters))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Prompt Template for Summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "49RqsAhDxjFg"
      },
      "outputs": [],
      "source": [
        "summarization_prompt_template = \"\"\"Write an extensive summary of about of the following:\n",
        "\n",
        "{text}\n",
        "\n",
        "SUMMARY:\"\"\"\n",
        "\n",
        "summarization_prompt = PromptTemplate(template=summarization_prompt_template, input_variables=[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Function to Create Chapter Summaries using LLMs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ehe6iObnx4l9"
      },
      "outputs": [],
      "source": [
        "def create_chapter_summary(chapter):\n",
        "    \"\"\"\n",
        "    Creates a summary of a chapter using a large language model (LLM).\n",
        "\n",
        "    Args:\n",
        "        chapter: A Document object representing the chapter to summarize.\n",
        "\n",
        "    Returns:\n",
        "        A Document object containing the summary of the chapter.\n",
        "    \"\"\"\n",
        "\n",
        "    chapter_txt = chapter.page_content  # Extract chapter text\n",
        "    model_name = \"gpt-3.5-turbo-0125\"  # Specify LLM model\n",
        "    llm = ChatOpenAI(temperature=0, model_name=model_name)  # Create LLM instance\n",
        "    gpt_35_turbo_max_tokens = 16000  # Maximum token limit for the LLM\n",
        "    verbose = False  # Set to True for more detailed output\n",
        "\n",
        "    # Calculate number of tokens in the chapter text\n",
        "    num_tokens = num_tokens_from_string(chapter_txt, model_name)\n",
        "\n",
        "    # Choose appropriate chain type based on token count\n",
        "    if num_tokens < gpt_35_turbo_max_tokens:\n",
        "        chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=summarization_prompt, verbose=verbose)\n",
        "    else:\n",
        "        chain = load_summarize_chain(llm, chain_type=\"map_reduce\", map_prompt=summarization_prompt, combine_prompt=summarization_prompt, verbose=verbose)\n",
        "\n",
        "    start_time = monotonic()  # Start timer\n",
        "    doc_chapter = Document(page_content=chapter_txt)  # Create Document object for chapter\n",
        "    summary = chain.invoke([doc_chapter])  # Generate summary using the chain\n",
        "    print(f\"Chain type: {chain.__class__.__name__}\")  # Print chain type\n",
        "    print(f\"Run time: {monotonic() - start_time}\")  # Print execution time\n",
        "\n",
        "    # Clean up summary text\n",
        "    summary = replace_double_lines_with_one_line(summary[\"output_text\"])\n",
        "\n",
        "    # Create Document object for summary\n",
        "    doc_summary = Document(page_content=summary, metadata=chapter.metadata)\n",
        "\n",
        "    return doc_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Summaries for Each Chapter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4UBurLsMCHj",
        "outputId": "668932cf-02d8-446d-e0ed-3f85b30d3bba"
      },
      "outputs": [],
      "source": [
        "chapter_summaries = []\n",
        "for chapter in chapters:\n",
        "    chapter_summaries.append(create_chapter_summary(chapter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function to Encode a Book into a Vector Store using OpenAI Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9t1Kq6fGS0EA"
      },
      "outputs": [],
      "source": [
        "def encode_book(path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        path: The path to the PDF file.\n",
        "        chunk_size: The desired size of each text chunk.\n",
        "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded book content.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load PDF documents\n",
        "    loader = PyPDFLoader(path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    cleaned_texts = replace_t_with_space(texts)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
        "\n",
        "    return vectorstore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding Chapter Summaries into Vector Store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FHt9Y12gMp2z"
      },
      "outputs": [],
      "source": [
        "def encode_chapter_summaries(chapter_summaries):\n",
        "    \"\"\"\n",
        "    Encodes a list of chapter summaries into a vector store using OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        chapter_summaries: A list of Document objects representing the chapter summaries.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded chapter summaries.\n",
        "    \"\"\"\n",
        "\n",
        "    embeddings = OpenAIEmbeddings()  # Create OpenAI embeddings\n",
        "    chapter_summaries_vectorstore = FAISS.from_documents(chapter_summaries, embeddings)  # Create vector store\n",
        "    return chapter_summaries_vectorstore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Vector Stores and Retrievers for Book and Chapter Summaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "# ### IF VECTOR STORES ALREADY EXIST, LOAD THEM\n",
        "if os.path.exists(\"chunks_vector_store\") and os.path.exists(\"chapter_summaries_vector_store\"):\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    chunks_vector_store =  FAISS.load_local(\"chunks_vector_store\", embeddings, allow_dangerous_deserialization=True)\n",
        "    chapter_summaries_vector_store =  FAISS.load_local(\"chapter_summaries_vector_store\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "else:\n",
        "    chunks_vector_store = encode_book(hp_pdf_path, chunk_size=1000, chunk_overlap=200)\n",
        "    chapter_summaries_vector_store = encode_chapter_summaries(chapter_summaries)\n",
        "\n",
        "    chunks_vector_store.save_local(\"chunks_vector_store\") # save the chunks_vector_store\n",
        "    chapter_summaries_vector_store.save_local(\"chapter_summaries_vector_store\") # save the chapter_summaries_vector_store\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create retrievers from the vector stores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 1})     \n",
        "chapter_summaries_query_retriever = chapter_summaries_vector_store.as_retriever(search_kwargs={\"k\": 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create graph nodes and LLM function for the nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agrregate retrieved content as string context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_context_per_question(state):\n",
        " \n",
        "    # Retrieve relevant documents\n",
        "    print(\"Retrieving relevant chunks...\")\n",
        "    question = state[\"question\"]\n",
        "    docs = chunks_query_retriever.get_relevant_documents(question)\n",
        "\n",
        "    # Concatenate document content\n",
        "    context = \" \".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "    print(\"Retrieving relevant chapter summaries...\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    docs_summaries = chapter_summaries_query_retriever.get_relevant_documents(state[\"question\"])\n",
        "\n",
        "\n",
        "    # Concatenate chapter summaries with citation information\n",
        "    context_summaries = \"\".join(\n",
        "        f\"{doc.page_content} (Chapter {doc.metadata['chapter']})\" for doc in docs_summaries\n",
        "    )\n",
        "\n",
        "    all_contexts = context + context_summaries\n",
        "    all_contexts = escape_quotes(all_contexts)\n",
        "\n",
        "    return {\"context\": all_contexts, \"question\": question}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving relevant chunks...\n",
            "Retrieving relevant chapter summaries...\n",
            "{'context': 'who he is?”\\n      “Who?”\\n      “Harry Potter!”\\n      Harry heard the little girl’s voice.\\n      “Oh, Mom, can I go on the train and see him, Mom, eh please....”\\n      “You’ve already seen him, Ginny, and the poor boy isn’t something you\\ngoggle at in a zoo. Is he really, Fred? How do you know?”\\n      “Asked him. Saw his scar. It’s really there — like lightning.”\\n      “Poor dear — no wonder he was alone, I wondered. He was ever so\\npolite when he asked how to get onto the platform.”\\n      “Never mind that, do you think he remembers what You-Know-WhoIn Chapter Four of Harry Potter and the Sorcerer\\'s Stone, titled \\\\\"The Keeper of the Keys,\\\\\" Harry, Dudley, and the Dursleys are visited by a giant man named Hagrid, who reveals to Harry that he is a wizard and has been accepted to Hogwarts School of Witchcraft and Wizardry. Hagrid explains to Harry about his parents, who were killed by the dark wizard Voldemort when Harry was just a baby. He also tells Harry about the magical world and the significance of his survival against Voldemort\\'s curse.\\nHagrid shares with Harry the letter of acceptance to Hogwarts and helps him send a message to the headmaster, Dumbledore, about their meeting. The Dursleys are shocked and resistant to the idea of Harry attending Hogwarts, but Hagrid stands up for Harry and insists that he will be going to the school.\\nHagrid also reveals that he was expelled from Hogwarts in his third year but was allowed to stay on as the gamekeeper. He explains that he is not supposed to do magic but has been helping Harry behind the scenes. Hagrid then gives Harry his coat to sleep under and mentions that there may be some doormice in the pockets.\\nOverall, this chapter sets the stage for Harry\\'s journey into the magical world and introduces him to key elements of his past and future at Hogwarts. It also highlights the conflict between Harry\\'s newfound identity as a wizard and the resistance from his non-magical family. (Chapter 4)', 'question': 'who is harry?'}\n"
          ]
        }
      ],
      "source": [
        "state = {\"question\": \"who is harry?\"}\n",
        "context = retrieve_context_per_question(state)\n",
        "print(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM based function to distill only relevant retrieved content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "keep_only_relevant_content_prompt_template = \"\"\"you receive a query: {query} and retrieved docuemnts: {retrieved_documents} from a vector store.\n",
        " You need to filter the retrieved data and keep only the sentences that are relevant, but all of them.\n",
        " you should output the distilled content in a json format. \n",
        " REMEMBER: the output has to be a json containing ALL the relevant sentences, and not the answer to the query. {format_instructions}\"\"\"\n",
        "\n",
        "class QuestionAnswer(BaseModel):\n",
        "    relevant_content: str = Field(description=\"The relevant content from the retrieved documents that is relevant to the query.\")\n",
        "\n",
        "\n",
        "json_parser = JsonOutputParser(pydantic_object=QuestionAnswer)\n",
        "\n",
        "keep_only_relevant_content_prompt = PromptTemplate(\n",
        "    template=keep_only_relevant_content_prompt_template,\n",
        "    input_variables=[\"query\", \"retrieved_documents\"],\n",
        "    partial_variables={\"format_instructions\": json_parser.get_format_instructions()}, \n",
        ")\n",
        "\n",
        "keep_only_relevant_content_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
        "keep_only_relevant_content_chain = keep_only_relevant_content_prompt | keep_only_relevant_content_llm | json_parser\n",
        "\n",
        "def keep_only_relevant_content(state):\n",
        "    \"\"\"\n",
        "    Keeps only the relevant content from the retrieved documents that is relevant to the query.\n",
        "\n",
        "    Args:\n",
        "        question: The query question.\n",
        "        context: The retrieved documents.\n",
        "        chain: The LLMChain instance.\n",
        "\n",
        "    Returns:\n",
        "        The relevant content from the retrieved documents that is relevant to the query.\n",
        "    \"\"\"\n",
        "    question = state[\"question\"]\n",
        "    context = state[\"context\"]\n",
        "\n",
        "    input_data = {\n",
        "    \"query\": question,\n",
        "    \"retrieved_documents\": context\n",
        "}\n",
        "    print(\"keeping only the relevant content...\")\n",
        "    # Invoke the chain to keep only the relevant content\n",
        "    output = keep_only_relevant_content_chain.invoke(input_data)\n",
        "    relevant_content = output[\"relevant_content\"]\n",
        "    relevant_content = \". \".join(relevant_content)\n",
        "    relevant_content = escape_quotes(relevant_content)\n",
        "\n",
        "    return {\"context\": relevant_content, \"question\": question}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "keeping only the relevant content...\n",
            "{'context': 'Harry heard the little girl’s voice.. “Harry Potter!”. Hagrid shares with Harry the letter of acceptance to Hogwarts and helps him send a message to the headmaster, Dumbledore, about their meeting.. Hagrid explains to Harry about his parents, who were killed by the dark wizard Voldemort when Harry was just a baby.. Hagrid tells Harry about the magical world and the significance of his survival against Voldemort’s curse.', 'question': 'who is harry?'}\n"
          ]
        }
      ],
      "source": [
        "state['context'] = context\n",
        "relevant_content_state = keep_only_relevant_content(state)\n",
        "print(relevant_content_state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM based function to re-write a question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Question Re-writer\n",
        "\n",
        "class RewriteQuestion(BaseModel):\n",
        "    \"\"\"\n",
        "    Output schema for the rewritten question.\n",
        "    \"\"\"\n",
        "    rewritten_question: str = Field(description=\"The improved question optimized for vectorstore retrieval.\")\n",
        "\n",
        "rewrite_question_string_parser = JsonOutputParser(pydantic_object=RewriteQuestion)\n",
        "\n",
        "\n",
        "rewrite_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
        "rewrite_prompt_template = \"\"\"You are a question re-writer that converts an input question to a better version optimized for vectorstore retrieval.\n",
        " Analyze the input question {question} and try to reason about the underlying semantic intent / meaning.\n",
        " {format_instructions}\n",
        " \"\"\"\n",
        "\n",
        "# Prompt\n",
        "\n",
        "rewrite_prompt = PromptTemplate(\n",
        "    template=rewrite_prompt_template,\n",
        "    input_variables=[\"question\"],\n",
        "    partial_variables={\"format_instructions\": rewrite_question_string_parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "question_rewriter = rewrite_prompt | rewrite_llm | rewrite_question_string_parser  # Combine prompt, LLM, and parser\n",
        "\n",
        "def rewrite_question(state):\n",
        "    \"\"\"Rewrites the given question using the LLM.\"\"\"\n",
        "    question = state[\"question\"]\n",
        "    print(\"Rewriting the question...\")\n",
        "    result = question_rewriter.invoke({\"question\": question})\n",
        "    new_question = result[\"rewritten_question\"]\n",
        "    return {\"question\": new_question}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM based function to answer a question given context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QuestionAnswerFromContext(BaseModel):\n",
        "    answer_based_on_content: str = Field(description=\"generates an answer to a query based on a given context.\")\n",
        "\n",
        "question_answer_from_context_json_parser = JsonOutputParser(pydantic_object=QuestionAnswerFromContext)\n",
        "question_answer_from_context_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
        "\n",
        "question_answer_from_context_prompt_template = \"\"\"you receive a query: {query} and a context: {context}. \n",
        "You need to answer the query from the context. the output has to be a json containing the answer to the query.\n",
        " {format_instructions}\"\"\"\n",
        "\n",
        "question_answer_from_context_prompt = PromptTemplate(\n",
        "    template=question_answer_from_context_prompt_template,\n",
        "    input_variables=[\"query\", \"context\"],\n",
        "    partial_variables={\"format_instructions\": question_answer_from_context_json_parser.get_format_instructions()},\n",
        ")\n",
        "question_answer_from_context_chain = question_answer_from_context_prompt | question_answer_from_context_llm | question_answer_from_context_json_parser\n",
        "\n",
        "def answer_question_from_context(state):\n",
        "    \"\"\"\n",
        "    Answers a question from a given context.\n",
        "\n",
        "    Args:\n",
        "        question: The query question.\n",
        "        context: The context to answer the question from.\n",
        "        chain: The LLMChain instance.\n",
        "\n",
        "    Returns:\n",
        "        The answer to the question from the context.\n",
        "    \"\"\"\n",
        "    question = state[\"question\"]\n",
        "    context = state[\"context\"]\n",
        "\n",
        "    input_data = {\n",
        "    \"query\": question,\n",
        "    \"context\": context\n",
        "}\n",
        "    print(\"Answering the question from the retrieved context...\")\n",
        "\n",
        "    # Invoke the chain to answer the question from the context\n",
        "    output = question_answer_from_context_chain.invoke(input_data)\n",
        "    answer = output[\"answer_based_on_content\"]\n",
        "    return {\"answer\": answer, \"context\": context, \"question\": question}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answering the question from the retrieved context...\n",
            "{'answer': 'Harry is the boy who survived a killing curse from Voldemort when he was a baby, and is now receiving a letter of acceptance to Hogwarts School of Witchcraft and Wizardry.', 'context': 'Harry heard the little girl’s voice.. “Harry Potter!”. Hagrid shares with Harry the letter of acceptance to Hogwarts and helps him send a message to the headmaster, Dumbledore, about their meeting.. Hagrid explains to Harry about his parents, who were killed by the dark wizard Voldemort when Harry was just a baby.. Hagrid tells Harry about the magical world and the significance of his survival against Voldemort’s curse.', 'question': 'who is harry?'}\n"
          ]
        }
      ],
      "source": [
        "answer_state = answer_question_from_context(relevant_content_state)\n",
        "print(answer_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create graph edges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM based function to determine if retrieved content is relevant to question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "is_relevant_content_prompt_template = \"\"\"you receive a query: {query} and a context: {context} retrieved from a vector store. \n",
        "You need to determine if the document is relevant to the query. \n",
        "\n",
        "{format_instructions}\"\"\"\n",
        "\n",
        "class Relevance(BaseModel):\n",
        "    is_relevant: bool = Field(description=\"Whether the document is relevant to the query.\")\n",
        "    explanation: str = Field(description=\"An explanation of why the document is relevant or not.\")\n",
        "\n",
        "is_relevant_json_parser = JsonOutputParser(pydantic_object=Relevance)\n",
        "is_relevant_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
        "\n",
        "is_relevant_content_prompt = PromptTemplate(\n",
        "    template=is_relevant_content_prompt_template,\n",
        "    input_variables=[\"query\", \"context\"],\n",
        "    partial_variables={\"format_instructions\": is_relevant_json_parser.get_format_instructions()},\n",
        ")\n",
        "is_relevant_content_chain = is_relevant_content_prompt | is_relevant_llm | is_relevant_json_parser\n",
        "\n",
        "def is_relevant_content(state):\n",
        "    \"\"\"\n",
        "    Determines if the document is relevant to the query.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    question = state[\"question\"]\n",
        "    context = state[\"context\"]\n",
        "\n",
        "    input_data = {\n",
        "    \"query\": question,\n",
        "    \"context\": context\n",
        "}\n",
        "\n",
        "    # Invoke the chain to determine if the document is relevant\n",
        "    output = is_relevant_content_chain.invoke(input_data)\n",
        "    print(\"Determining if the document is relevant...\")\n",
        "    if output[\"is_relevant\"] == True:\n",
        "        print(\"The document is relevant.\")\n",
        "        return \"relevant\"\n",
        "    else:\n",
        "        print(\"The document is not relevant.\")\n",
        "        return \"not relevant\"\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Determining if the document is relevant...\n",
            "The document is relevant.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "state = {\"context\": relevant_content_state[\"context\"], \"question\": relevant_content_state[\"question\"]}\n",
        "output = is_relevant_content(state)\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM chain to check if an answer is hallucination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class is_grounded_on_facts(BaseModel):\n",
        "    \"\"\"\n",
        "    Output schema for the rewritten question.\n",
        "    \"\"\"\n",
        "    grounded_on_facts: bool = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n",
        "    explanation: str = Field(description=\"An explanation of why the answer is grounded in the facts or not.\")\n",
        "\n",
        "grounded_on_facts_parser = JsonOutputParser(pydantic_object=is_grounded_on_facts)\n",
        "is_grounded_on_facts_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
        "is_grounded_on_facts_prompt_template = \"\"\"You are a fact-checker that determines if the answer to the question is grounded in the facts.\n",
        " Analyze the input context {context} and the answer {answer} and determine if the answer is grounded in the facts.\n",
        " {format_instructions}\n",
        " \"\"\"\n",
        "is_grounded_on_facts_prompt = PromptTemplate(\n",
        "    template=is_grounded_on_facts_prompt_template,\n",
        "    input_variables=[\"context\", \"answer\"],\n",
        "    partial_variables={\"format_instructions\": grounded_on_facts_parser.get_format_instructions()},\n",
        ")\n",
        "is_grounded_on_facts_chain = is_grounded_on_facts_prompt | is_grounded_on_facts_llm | grounded_on_facts_parser\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM chain to determine if a question can be fully answered given a context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "can_be_answered_prompt_template = \"\"\"You receive a query: {question} and a context: {context}. \n",
        "You need to determine if the question can be fully answered based on the context.\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "class QuestionAnswer(BaseModel):\n",
        "    can_be_answered: bool = Field(description=\"binary result of whether the question can be fully answered or not\")\n",
        "    explanation: str = Field(description=\"An explanation of why the question can be fully answered or not.\")\n",
        "\n",
        "can_be_answered_json_parser = JsonOutputParser(pydantic_object=QuestionAnswer)\n",
        "\n",
        "answer_question_prompt = PromptTemplate(\n",
        "    template=can_be_answered_prompt_template,\n",
        "    input_variables=[\"question\",\"context\"],\n",
        "    partial_variables={\"format_instructions\": can_be_answered_json_parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "can_be_answered_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
        "can_be_answered_chain = answer_question_prompt | can_be_answered_llm | can_be_answered_json_parser\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### function to check both cases - hallucination and full answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def grade_generation_v_documents_and_question(state):\n",
        "    \"\"\"Determines if the answer to the question is grounded in the facts.\"\"\"\n",
        "    print(\"Checking if the answer is grounded in the facts...\")\n",
        "    context = state[\"context\"]\n",
        "    answer = state[\"answer\"]\n",
        "    question = state[\"question\"]\n",
        "    \n",
        "    result = is_grounded_on_facts_chain.invoke({\"context\": context, \"answer\": answer})\n",
        "    grounded_on_facts = result[\"grounded_on_facts\"]\n",
        "    if not grounded_on_facts:\n",
        "        print(\"The answer is hallucination.\")\n",
        "        return \"hallucination\"\n",
        "    else:\n",
        "        print(\"The answer is grounded in the facts.\")\n",
        "\n",
        "        input_data = {\n",
        "            \"question\": question,\n",
        "            \"context\": context\n",
        "        }\n",
        "\n",
        "        # Invoke the chain to determine if the question can be answered\n",
        "        print(\"Determining if the question is fully answered...\")\n",
        "        output = can_be_answered_chain.invoke(input_data)\n",
        "        can_be_answered = output[\"can_be_answered\"]\n",
        "        if can_be_answered == True:\n",
        "            print(\"The question can be fully answered.\")\n",
        "            return \"useful\"\n",
        "        else:\n",
        "            print(\"The question cannot be fully answered.\")\n",
        "            return \"not_useful\"\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test a pipeline of all parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "init_state = {\"question\": \"who is harry?\"}\n",
        "context_state = retrieve_context_per_question(state)\n",
        "relevant_content_state = keep_only_relevant_content(context_state)\n",
        "is_relevant_content_state = is_relevant_content(relevant_content_state)\n",
        "answer_state = answer_question_from_context(relevant_content_state)\n",
        "final_answer = grade_generation_v_documents_and_question(answer_state)\n",
        "print(final_answer[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import END, StateGraph\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        context: context\n",
        "        answer: answer\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    context: str\n",
        "    answer: str\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve_context_per_question\",retrieve_context_per_question)\n",
        "workflow.add_node(\"keep_only_relevant_content\",keep_only_relevant_content)\n",
        "workflow.add_node(\"rewrite_question\",rewrite_question)\n",
        "workflow.add_node(\"answer_question_from_context\",answer_question_from_context)\n",
        "\n",
        "# Build the graph\n",
        "workflow.set_entry_point(\"retrieve_context_per_question\")\n",
        "workflow.add_edge(\"retrieve_context_per_question\", \"keep_only_relevant_content\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"keep_only_relevant_content\",\n",
        "    is_relevant_content,\n",
        "    {\"relevant\":\"answer_question_from_context\",\n",
        "      \"not relevant\":\"rewrite_question\"},\n",
        "    )\n",
        "workflow.add_edge(\"rewrite_question\", \"retrieve_context_per_question\")\n",
        "workflow.add_conditional_edges(\n",
        "\"answer_question_from_context\",\n",
        "grade_generation_v_documents_and_question,\n",
        "{\"hallucination\":\"answer_question_from_context\",\n",
        "\"not_useful\":\"rewrite_question\",\n",
        "\"useful\":END},\n",
        "\n",
        ")\n",
        "\n",
        "retrival_app = workflow.compile()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain_core.runnables.graph import MermaidDrawMethod\n",
        "from IPython.display import display, Image\n",
        "\n",
        "display(\n",
        "    Image(\n",
        "        retrival_app.get_graph().draw_mermaid_png(\n",
        "            draw_method=MermaidDrawMethod.API,\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from langchain.schema import AIMessage\n",
        "\n",
        "def run_retrieval_graph(question: str):\n",
        "    inputs = {\"question\": question}\n",
        "    for output in retrival_app.stream(inputs):\n",
        "        for key, value in output.items():\n",
        "            pass  # Node\n",
        "            # ... (your existing code)\n",
        "        pprint(\"--------------------\")\n",
        "    print(value)\n",
        "    return AIMessage(\n",
        "        content=value,  # The actual answer\n",
        "    )\n",
        "\n",
        "# run_retrieval_graph(\"who is harry?\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tools = [\n",
        "#     Tool(\n",
        "#         name = \"retrieval_graph\",\n",
        "#         description=\"A graph that retrieves relevant information from a vector store based on a given question.\",\n",
        "#         func = lambda query: run_retrieval_graph(query),\n",
        "#     )\n",
        "# ]\n",
        "\n",
        "### TODO: the tools below is for debugging only, change it to the above tools\n",
        "\n",
        "def run_retrieve_context(question: str):\n",
        "    inputs = {\"question\": question}\n",
        "    state = retrieve_context_per_question(inputs)\n",
        "    return state\n",
        "tools = [\n",
        "Tool(\n",
        "    name=\"RetrieveChapterSummary\",\n",
        "    func=lambda question: run_retrieve_context(question),\n",
        "    description=\"Retrieves relevant chapter summaries based on a question.\",\n",
        "),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m System Message \u001b[0m================================\n",
            "\n",
            "You are a helpful assistant.\n",
            "\n",
            "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
            "\n",
            "\u001b[33;1m\u001b[1;3m{{messages}}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from langchain import hub\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# Get the prompt to use - you can modify this!\n",
        "prompt = hub.pull(\"wfh/react-agent-executor\")\n",
        "prompt.pretty_print()\n",
        "\n",
        "# Choose the LLM that will drive the agent\n",
        "agent_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=2000)\n",
        "agent_executor = create_react_agent(agent_llm, tools, messages_modifier=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['{messages}'], input_types={'{messages}': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, metadata={'lc_hub_owner': 'wfh', 'lc_hub_repo': 'react-agent-executor', 'lc_hub_commit_hash': 'bccfbbc5de8559d19d44c8ea2229bb6d06c99e402ea29b8694b294a31730a7a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant.')), MessagesPlaceholder(variable_name='{messages}')])"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='who is harry?', id='17551bd3-795d-40bb-a5ab-69f1c6c90dd9'),\n",
              "  AIMessage(content=\"There are several notable individuals named Harry, so it's not clear which one you're referring to. Here are a few possibilities:\\n\\n* Prince Harry, Duke of Sussex, a member of the British royal family\\n* Harry Potter, the fictional protagonist of J.K. Rowling's popular book series\\n* Harry Styles, a British singer, songwriter, and actor who was a member of the boy band One Direction\\n* Harry Truman, the 33rd President of the United States\\n\\nIf you could provide more context or clarify which Harry you're referring to, I'd be happy to try and help!\", response_metadata={'token_usage': {'completion_time': 0.392, 'completion_tokens': 120, 'prompt_time': 0.436, 'prompt_tokens': 903, 'queue_time': None, 'total_time': 0.8280000000000001, 'total_tokens': 1023}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_c1a4bcec29', 'finish_reason': 'stop', 'logprobs': None}, id='run-5c23c6d9-bd8e-43ed-9d53-0a8d8eee3f4c-0')]}"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke({\"messages\": [(\"user\", \"who is harry?\")]})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Annotated, TypedDict\n",
        "import operator\n",
        "\n",
        "\n",
        "class PlanExecute(TypedDict):\n",
        "    input: str\n",
        "    plan: List[str]\n",
        "    past_steps: Annotated[List[Tuple], operator.add]\n",
        "    response: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "class Plan(BaseModel):\n",
        "    \"\"\"Plan to follow in future\"\"\"\n",
        "\n",
        "    steps: List[str] = Field(\n",
        "        description=\"different steps to follow, should be in sorted order\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The method `ChatGroq.with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
            "  warn_beta(\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "planner_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
        "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
        "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
        "you have no prior information about the question. all you have is access to a vector store of chunks from some book and a vector store of the same book chapter summaries.\n",
        "you don't know what book it is and cannot rely on any prior knowledge of that book even if you realize what book it is.\"\"\"\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "planner = planner_prompt | ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=2000).with_structured_output(Plan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Plan(steps=[\"Determine the circumstances of Quirrell's possession\", \"Understand the nature of Voldemort's influence\", \"Discover the role of the Philosopher's Stone\", \"Recognize Harry's courage and loyalty\", \"Conclude how Harry's actions led to Quirrell's defeat\"])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "planner.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            (\"user\", \"how did harry beat quirrell?\")\n",
        "        ]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Union\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "class Response(BaseModel):\n",
        "    \"\"\"Response to user.\"\"\"\n",
        "\n",
        "    response: str\n",
        "\n",
        "\n",
        "class Act(BaseModel):\n",
        "    \"\"\"Action to perform.\"\"\"\n",
        "\n",
        "    action: Union[Response, Plan] = Field(\n",
        "        description=\"Action to perform. If you want to respond to user, use Response. \"\n",
        "        \"If you need to further use tools to get the answer, use Plan.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class ActPossibleResults(BaseModel):\n",
        "    \"\"\"Possible results of the action.\"\"\"\n",
        "    response: Response = Field(description=\"Response to user.\")\n",
        "    plan: Plan = Field(description=\"Plan to follow in future.\")\n",
        "\n",
        "act_possible_results_parser = JsonOutputParser(pydantic_object=ActPossibleResults)\n",
        "\n",
        "# possible_answer_parser = StructuredOutputParser(pydantic_object=ActPossibleResults)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# replanner_prompt = ChatPromptTemplate.from_template(\n",
        "#     \"\"\"you have no prior information about the question. all you have is access to a vector store of chunks from some book and a vector store of the same book chapter summaries.\n",
        "# you don't know what book it is and cannot rely on any prior knowledge of that book even if you realize what book it is.\n",
        "#     For the given objective, come up with a simple step by step plan. \\\n",
        "# This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
        "# The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
        "\n",
        "\n",
        "# Your objective was this:\n",
        "# {input}\n",
        "\n",
        "# Your original plan was this:\n",
        "# {plan}\n",
        "\n",
        "# You have currently done the follow steps:\n",
        "# {past_steps}\n",
        "\n",
        "# Update your plan accordingly. If no more steps are needed and you can return to the user,\n",
        "# then respond with only the final answer. If further steps are needed, fill out the plan with only those steps.\n",
        "# Do not return previously done steps as part of the plan.\n",
        "\n",
        "# The output should be in the following format:\n",
        "\n",
        "# if you think you have the final answer and decide to respond to the user, output:\n",
        "# ```\n",
        "# response:'your response here'\n",
        "# ```\n",
        "\n",
        "# else, if you need to further use tools to get the answer, output:\n",
        "# ```\n",
        "# plan: [step1, step2, step3, ...]) \n",
        "# ```\n",
        "\n",
        "# \"\"\"\n",
        "# )\n",
        "\n",
        "\n",
        "replanner_prompt_teomplate = \"\"\"you have no prior information about the question. all you have is access to a vector store of chunks from some book and a vector store of the same book chapter summaries.\n",
        "you don't know what book it is and cannot rely on any prior knowledge of that book even if you realize what book it is.\n",
        "    For the given objective, come up with a simple step by step plan. \\\n",
        "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
        "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
        "\n",
        "\n",
        "Your objective was this:\n",
        "{input}\n",
        "\n",
        "Your original plan was this:\n",
        "{plan}\n",
        "\n",
        "You have currently done the follow steps:\n",
        "{past_steps}\n",
        "\n",
        "Update your plan accordingly. If no more steps are needed and you can return to the user,\n",
        "then respond with only the final answer. If further steps are needed, fill out the plan with only those steps.\n",
        "Do not return previously done steps as part of the plan.\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "replanner_prompt = PromptTemplate(\n",
        "    template=replanner_prompt_teomplate,\n",
        "    input_variables=[\"input\", \"plan\", \"past_steps\"],\n",
        "    partial_variables={\"format_instructions\": act_possible_results_parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "replanner = replanner_prompt | ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=2000)\n",
        "\n",
        "\n",
        "answer_question_prompt = PromptTemplate(\n",
        "    template=can_be_answered_prompt_template,\n",
        "    input_variables=[\"question\",\"context\"],\n",
        "    partial_variables={\"format_instructions\": can_be_answered_json_parser.get_format_instructions()},\n",
        ")\n",
        "# async def replan_step(state: PlanExecute):\n",
        "#     print(\"Replanning step\")\n",
        "#     output = await replanner.ainvoke(state)\n",
        "#     if isinstance(output.action, Response):\n",
        "#         return {\"response\": output.action.response}\n",
        "#     else:\n",
        "#         return {\"plan\": output.action.steps}\n",
        "\n",
        "\n",
        "replanner = replanner_prompt | ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=2000) | act_possible_results_parser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_response_or_plan(text):\n",
        "  \"\"\"\n",
        "  Extracts either the response or plan from a string, if present.\n",
        "\n",
        "  Args:\n",
        "    text: The input string.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary with the key \"response\" or \"plan\" and the value being the part of the string after \"response:\" or \"plan:\" with double quotes,\n",
        "    or None if the string doesn't start with either \"response:\" or \"plan:\".\n",
        "  \"\"\"\n",
        "\n",
        "  if text.startswith(\"response:\"):\n",
        "    response = text[9:].strip()\n",
        "    return {\"response\": response}\n",
        "  elif text.startswith(\"plan:\"):\n",
        "    plan = text[5:].strip()  # Extract the part after \"plan:\"\n",
        "    return {\"plan\": plan}\n",
        "  else:\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "state = {'input': 'who is harry?', 'plan': ['Determine who Harry is', 'Search the vector store of chunks from the book for mentions of Harry', 'Search the vector store of chapter summaries for mentions of Harry', 'Combine the results to determine who Harry is'],'past_steps': ('Determine who Harry is', 'Based on the results, it seems that Harry is a student at Hogwarts School of Witchcraft and Wizardry,and the context provided is from Chapter 10 of Harry Potter and the Sorcerer\\'s Stone.'), 'response': None}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'response': {'response': 'Harry is a student at Hogwarts School of Witchcraft and Wizardry.'}, 'plan': {'steps': []}}\n"
          ]
        }
      ],
      "source": [
        "# res = replanner_unstrctured.invoke(state)\n",
        "# structured = res.with_structured_output(Act)\n",
        "# print(res)\n",
        "# print(structured)\n",
        "res = replanner.invoke(state)\n",
        "print(res)\n",
        "# return_value = extract_response_or_plan(res.content)\n",
        "# print(return_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "state2 = {'input': 'how did harry beat quirrell?', 'plan': [\"Determine the circumstances of Quirrell's possession\", \"Understand the nature of Voldemort's influence\", \"Discover the role of the Philosopher's Stone\", \"Recognize Harry's courage and loyalty\", \"Conclude how Harry's actions led to Quirrell's defeat\"], 'past_steps': (\"Determine the circumstances of Quirrell's possession\", \"I have determined the circumstances of Quirrell's possession. Quirrell was possessed by Lord Voldemort, who was sharing his body.\"), 'response': None}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'response': {'response': ''}, 'plan': {'steps': [\"Understand the nature of Voldemort's influence\", \"Discover the role of the Philosopher's Stone\", \"Recognize Harry's courage and loyalty\", \"Conclude how Harry's actions led to Quirrell's defeat\"]}}\n"
          ]
        }
      ],
      "source": [
        "res2 = replanner.invoke(state2)\n",
        "print(res2)\n",
        "# return_value = extract_response_or_plan(res2.content)\n",
        "# print(return_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "\n",
        "\n",
        "# async def execute_step(state: PlanExecute):\n",
        "#     print(\"Executing step\")\n",
        "#     plan = state[\"plan\"]\n",
        "#     plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
        "#     task = plan[0]\n",
        "#     task_formatted = f\"\"\"For the following plan:\n",
        "# {plan_str}\\n\\nYou are tasked with executing step {1}, {task}.\"\"\"\n",
        "#     agent_response = await agent_executor.ainvoke(\n",
        "#         {\"messages\": [(\"user\", task_formatted)]}\n",
        "#     )\n",
        "#     return {\n",
        "#         \"past_steps\": (task, agent_response[\"messages\"][-1].content),\n",
        "#     }\n",
        "\n",
        "def execute_step(state: PlanExecute):\n",
        "    print(\"Executing step\")\n",
        "    plan = state[\"plan\"]\n",
        "    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
        "    task = plan[0]\n",
        "    task_formatted = f\"\"\"For the following plan:\n",
        "{plan_str}\\n\\nYou are tasked with executing step {1}, {task}.\"\"\"\n",
        "    agent_response = agent_executor.invoke(\n",
        "        {\"messages\": [(\"user\", task_formatted)]}\n",
        "    )\n",
        "    return {\n",
        "        \"past_steps\": (task, agent_response[\"messages\"][-1].content),\n",
        "    }\n",
        "\n",
        "\n",
        "# async def plan_step(state: PlanExecute):\n",
        "#     print(\"Planning step\")\n",
        "#     plan = await planner.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
        "#     return {\"plan\": plan.steps}\n",
        "\n",
        "def plan_step(state: PlanExecute):\n",
        "    print(\"Planning step\")\n",
        "    plan = planner.invoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
        "    return {\"plan\": plan.steps}\n",
        "\n",
        "\n",
        "# async def replan_step(state: PlanExecute):\n",
        "#     print(\"Replanning step\")\n",
        "#     output = await replanner.ainvoke(state)\n",
        "#     if isinstance(output.action, Response):\n",
        "#         return {\"response\": output.action.response}\n",
        "#     else:\n",
        "#         return {\"plan\": output.action.steps}\n",
        "\n",
        "\n",
        "def replan_step(state: PlanExecute):\n",
        "    print(\"Replanning step\")\n",
        "    output =  replanner.invoke(state)\n",
        "    output_content = output.content\n",
        "    return extract_response_or_plan(output_content)\n",
        "  \n",
        "\n",
        "\n",
        "# def replan_step(state: PlanExecute):\n",
        "#     print(\"Replanning step\")\n",
        "#     print(f'state:{state}')\n",
        "\n",
        "#     output = replanner.invoke(state)\n",
        "#     print(f'output:{output}')\n",
        "#     if isinstance(output.action, Response):\n",
        "#         return {\"response\": output.action.response}\n",
        "#     else:\n",
        "#         if output.action.steps == []:\n",
        "#             return {\"response\": state.past_steps[-1]}\n",
        "#         return {\"plan\": output.action.steps}\n",
        "#         # return {\"plan\": output.action.steps}\n",
        "\n",
        "\n",
        "def should_end(state: PlanExecute) -> Literal[\"agent\", \"__end__\"]:\n",
        "    print(\"Checking if should end\")\n",
        "    if \"response\" in state and state[\"response\"]:\n",
        "        return \"__end__\"\n",
        "    else:\n",
        "        return \"agent\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph\n",
        "\n",
        "agent_workflow = StateGraph(PlanExecute)\n",
        "\n",
        "# Add the plan node\n",
        "agent_workflow.add_node(\"planner\", plan_step)\n",
        "\n",
        "# Add the execution step\n",
        "agent_workflow.add_node(\"agent\", execute_step)\n",
        "\n",
        "# Add a replan node\n",
        "agent_workflow.add_node(\"replan\", replan_step)\n",
        "\n",
        "agent_workflow.set_entry_point(\"planner\")\n",
        "\n",
        "# From plan we go to agent\n",
        "agent_workflow.add_edge(\"planner\", \"agent\")\n",
        "\n",
        "# From agent, we replan\n",
        "agent_workflow.add_edge(\"agent\", \"replan\")\n",
        "\n",
        "agent_workflow.add_conditional_edges(\n",
        "    \"replan\",\n",
        "    # Next, we pass in the function that will determine which node is called next.\n",
        "    should_end,\n",
        ")\n",
        "\n",
        "# Finally, we compile it!\n",
        "# This compiles it into a LangChain Runnable,\n",
        "# meaning you can use it as you would any other runnable\n",
        "plan_and_execute_app = agent_workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(plan_and_execute_app.get_graph(xray=True).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# config = {\"recursion_limit\": 50}\n",
        "os.environ[\"PYDEVD_WARN_EVALUATION_TIMEOUT\"] = \"100000\"\n",
        "\n",
        "\n",
        "\n",
        "# async def run_plan_and_execute_app(inputs):\n",
        "#     for plan_output in plan_and_execute_app.stream(inputs):\n",
        "#         for agent_state_key, agent_state_value in plan_output.items():\n",
        "#             print(f' curr step: {agent_state_value}')\n",
        "#         pprint(\"--------------------\")\n",
        "#     return agent_state_value\n",
        "\n",
        "# inputs = {\"input\": \"how did harry beat quirrell?\"}\n",
        "# async for plan_output in plan_and_execute_app.astream(inputs):\n",
        "#         for agent_state_key, agent_state_value in plan_output.items():\n",
        "#             print(f' curr step: {agent_state_value}')\n",
        "#             pass  # Node\n",
        "#             # ... (your existing code)\n",
        "#         pprint(\"--------------------\")\n",
        "\n",
        "# async def execute_plan_and_print_steps(inputs):\n",
        "#     async for plan_output in plan_and_execute_app.astream(inputs):\n",
        "#         for agent_state_key, agent_state_value in plan_output.items():\n",
        "#             print(f' curr step: {agent_state_value}')\n",
        "#             pass  # Node\n",
        "#             # ... (your existing code)\n",
        "#         pprint(\"--------------------\")\n",
        "#     return agent_state_value\n",
        "\n",
        "def execute_plan_and_print_steps(inputs):\n",
        "    for plan_output in plan_and_execute_app.stream(inputs):\n",
        "        for agent_state_key, agent_state_value in plan_output.items():\n",
        "            print(f' curr step: {agent_state_value}')\n",
        "            pass  # Node\n",
        "            # ... (your existing code)\n",
        "        pprint(\"--------------------\")\n",
        "    return agent_state_value\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Planning step\n",
            " curr step: {'plan': ['Determine who Harry is', 'Search the vector store of chunks from the book for mentions of Harry', 'Search the vector store of chapter summaries for mentions of Harry', 'Combine the results to determine who Harry is']}\n",
            "'--------------------'\n",
            "Executing step\n",
            "Retrieving relevant chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving relevant chapter summaries...\n",
            "Retrieving relevant chunks...\n",
            "Retrieving relevant chapter summaries...\n"
          ]
        },
        {
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>{\"tool_calls\":[{\"id\":\"call_3r4t\",\"type\":\"function\",\"function\":{\"name\":\"SearchVectorStore\"},\"parameters\":{\"__arg1\":\"Harry\"}}]}</tool-use>'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[37], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwho is harry?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# final_answer =await execute_plan_and_print_steps(inputs)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m final_answer \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_plan_and_print_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe final answer is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_answer[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[36], line 31\u001b[0m, in \u001b[0;36mexecute_plan_and_print_steps\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_plan_and_print_steps\u001b[39m(inputs):\n\u001b[1;32m---> 31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mplan_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mplan_and_execute_app\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43magent_state_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_state_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mplan_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m curr step: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43magent_state_value\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:845\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m    838\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mwait(\n\u001b[0;32m    839\u001b[0m     futures,\n\u001b[0;32m    840\u001b[0m     return_when\u001b[38;5;241m=\u001b[39mconcurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[0;32m    841\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m    842\u001b[0m )\n\u001b[0;32m    844\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m--> 845\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;66;03m# combine pending writes from all tasks\u001b[39;00m\n\u001b[0;32m    848\u001b[0m pending_writes \u001b[38;5;241m=\u001b[39m deque[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]()\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1370\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[1;34m(done, inflight, step)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[0;32m   1369\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[1;32m-> 1370\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m   1371\u001b[0m         \u001b[38;5;66;03m# TODO this is where retry of an entire step would happen\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langgraph\\utils.py:88\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m     82\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, config)\n\u001b[0;32m     83\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     84\u001b[0m         {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: config}\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs\n\u001b[0;32m     87\u001b[0m     )\n\u001b[1;32m---> 88\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "Cell \u001b[1;32mIn[34], line 25\u001b[0m, in \u001b[0;36mexecute_step\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     22\u001b[0m     task \u001b[38;5;241m=\u001b[39m plan[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     23\u001b[0m     task_formatted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mFor the following plan:\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mplan_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYou are tasked with executing step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 25\u001b[0m     agent_response \u001b[38;5;241m=\u001b[39m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_formatted\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m: (task, agent_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent),\n\u001b[0;32m     30\u001b[0m     }\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1281\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1281\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:845\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m    838\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mwait(\n\u001b[0;32m    839\u001b[0m     futures,\n\u001b[0;32m    840\u001b[0m     return_when\u001b[38;5;241m=\u001b[39mconcurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[0;32m    841\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m    842\u001b[0m )\n\u001b[0;32m    844\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m--> 845\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;66;03m# combine pending writes from all tasks\u001b[39;00m\n\u001b[0;32m    848\u001b[0m pending_writes \u001b[38;5;241m=\u001b[39m deque[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]()\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1370\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[1;34m(done, inflight, step)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[0;32m   1369\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[1;32m-> 1370\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m   1371\u001b[0m         \u001b[38;5;66;03m# TODO this is where retry of an entire step would happen\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3963\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3961\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[39;00m\n\u001b[0;32m   3962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 3963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3964\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3965\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3966\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3967\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   3971\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3972\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3973\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1626\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1623\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[0;32m   1624\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1625\u001b[0m         Output,\n\u001b[1;32m-> 1626\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1628\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1629\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1630\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1634\u001b[0m     )\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1636\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3837\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   3835\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[0;32m   3836\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3837\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3838\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   3839\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3840\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[0;32m   3841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langgraph\\prebuilt\\chat_agent_executor.py:400\u001b[0m, in \u001b[0;36mcreate_react_agent.<locals>.call_model\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_model\u001b[39m(state: AgentState):\n\u001b[0;32m    399\u001b[0m     messages \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 400\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_runnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last_step\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtool_calls:\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    403\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m    404\u001b[0m                 AIMessage(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    408\u001b[0m             ]\n\u001b[0;32m    409\u001b[0m         }\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4525\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   4519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   4520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   4522\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   4523\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   4524\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 4525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4526\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4527\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4528\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:158\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    155\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    157\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 158\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    168\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:560\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    554\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    558\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    559\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:421\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    420\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 421\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    422\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    423\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    425\u001b[0m ]\n\u001b[0;32m    426\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:411\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 411\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m         )\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:632\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    636\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\langchain_groq\\chat_models.py:242\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    238\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    241\u001b[0m }\n\u001b[1;32m--> 242\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:178\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    Creates a completion for a chat prompt\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1194\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1182\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1191\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1192\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1193\u001b[0m     )\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\groq\\_base_client.py:896\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    889\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    894\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    895\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG-Harry-Potter\\.venv\\Lib\\site-packages\\groq\\_base_client.py:987\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    984\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    986\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 987\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    990\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    991\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    994\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    995\u001b[0m )\n",
            "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>{\"tool_calls\":[{\"id\":\"call_3r4t\",\"type\":\"function\",\"function\":{\"name\":\"SearchVectorStore\"},\"parameters\":{\"__arg1\":\"Harry\"}}]}</tool-use>'}}"
          ]
        }
      ],
      "source": [
        "\n",
        "inputs = {\"input\": \"who is harry?\"}\n",
        "# final_answer =await execute_plan_and_print_steps(inputs)\n",
        "final_answer = execute_plan_and_print_steps(inputs)\n",
        "\n",
        "\n",
        "print(f'The final answer is: {final_answer[\"response\"]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Planning step\n",
            " curr step: {'plan': ['Determine the context of the question', 'Retrieve relevant information from the vector stores', 'Analyze the information to find the answer', 'Provide the answer']}\n",
            "'--------------------'\n",
            "Executing step\n",
            "Retrieving relevant chunks...\n",
            "Retrieving relevant chapter summaries...\n",
            " curr step: {'past_steps': ('Determine the context of the question', 'Based on the context provided, I will proceed to step 2, Retrieve relevant information from the vector stores.')}\n",
            "'--------------------'\n",
            "Replanning step\n",
            "Checking if should end\n",
            " curr step: {'plan': \"steps=['Step 1: Retrieve relevant information from the vector stores related to Harry and Quirrell', 'Step 2: Analyze the retrieved information to find the specific event or scene where Harry beats Quirrell', 'Step 3: Extract the details of how Harry beat Quirrell from the analyzed information']\"}\n",
            "'--------------------'\n",
            "Executing step\n",
            "Retrieving relevant chunks...\n",
            "Retrieving relevant chapter summaries...\n",
            "Retrieving relevant chunks...\n",
            "Retrieving relevant chapter summaries...\n",
            "Retrieving relevant chunks...\n",
            "Retrieving relevant chapter summaries...\n",
            "Retrieving relevant chunks...\n",
            "Retrieving relevant chapter summaries...\n",
            "Retrieving relevant chunks...\n",
            "Retrieving relevant chapter summaries...\n",
            " curr step: {'past_steps': ('s', \"It seems like we've reached a loop where the tool is yielding the same result repeatedly. I'll stop here and not call the tool again.\")}\n",
            "'--------------------'\n",
            "Replanning step\n",
            "Checking if should end\n",
            " curr step: {'plan': \"steps=['Step 1: Re-analyze the previously retrieved information to identify the specific event or scene where Harry beats Quirrell', 'Step 2: Extract the details of how Harry beat Quirrell from the analyzed information']\"}\n",
            "'--------------------'\n",
            "Executing step\n",
            "Retrieving relevant chunks...\n",
            "Retrieving relevant chapter summaries...\n",
            "Retrieving relevant chunks...\n",
            "Retrieving relevant chapter summaries...\n",
            "Retrieving relevant chunks...\n",
            "Retrieving relevant chapter summaries...\n",
            "Retrieving relevant chunks...\n",
            "Retrieving relevant chapter summaries...\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"input\": \"how did harry beat quirrell?\"}\n",
        "final_answer = execute_plan_and_print_steps(inputs)\n",
        "\n",
        "print(f'The final answer is: {final_answer[\"response\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Template for Answering Questions Using Context-Specific Information\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBKQkR-W4ZyV"
      },
      "source": [
        "### Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME52zvjPNv47"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"Who gave Harry Potter his first broomstick?\",\n",
        "    \"What is the name of the three-headed dog guarding the Sorcerer's Stone?\",\n",
        "    \"Which house did the Sorting Hat initially consider for Harry?\",\n",
        "    \"What is the name of Harry's owl?\"\n",
        "]\n",
        "#     \"How did Harry and his friends get past Fluffy?\",\n",
        "#     \"What is the Mirror of Erised?\",\n",
        "#     \"Who tried to steal the Sorcerer's Stone?\",\n",
        "#     \"How did Harry defeat Quirrell/Voldemort?\",\n",
        "#     \"What is Harry's parent's secret weapon against Voldemort?\",\n",
        "# ]\n",
        "\n",
        "ground_truth_answers = [\n",
        "    \"Professor McGonagall\",\n",
        "    \"Fluffy\",\n",
        "    \"Slytherin\",\n",
        "    \"Hedwig\",\n",
        "    # \"They played music to put Fluffy to sleep.\",\n",
        "    # \"A magical mirror that shows the 'deepest, most desperate desire of our hearts.'\",\n",
        "    # \"Professor Quirrell, possessed by Voldemort\",\n",
        "    # \"Harry's mother's love protected him, causing Quirrell/Voldemort pain when they touched him.\",\n",
        "    # \"Love\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Answers and Retrieving Documents for Predefined Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "Wo9EEV0j4mJA",
        "outputId": "14c46214-8fb8-41aa-9845-73367d9a738f"
      },
      "outputs": [],
      "source": [
        "generated_answers = []\n",
        "retrieved_documents = []\n",
        "for question in questions:\n",
        "    result, all_context_book, all_context_summaries = answer_question_pipeline(question, chunks_retriever, chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm)\n",
        "    generated_answers.append(result['text'])\n",
        "    retrieved_documents.append(all_context_book + all_context_summaries)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Displaying Retrieved Documents and Generated Answers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'retrieved_documents: {retrieved_documents}\\n')\n",
        "print(f'generated_answers: {generated_answers}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing Data and Conducting Ragas Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for Ragas evaluation\n",
        "data_samples = {\n",
        "    'question': questions,  # Replace with your list of questions\n",
        "    'answer': generated_answers,  # Replace with your list of generated answers\n",
        "    'contexts': retrieved_documents,  # Your retrieved_documents list\n",
        "    'ground_truth': ground_truth_answers  # Replace with your list of ground truth answers\n",
        "}\n",
        "\n",
        "# Convert contexts to list of strings (if necessary)\n",
        "data_samples['contexts'] = [list(context) for context in data_samples['contexts']]\n",
        "\n",
        "dataset = Dataset.from_dict(data_samples)\n",
        "\n",
        "# Evaluate using Ragas with the specified metrics\n",
        "metrics = [\n",
        "    answer_correctness,\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    answer_similarity\n",
        "]\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-1106-preview\", max_tokens=4000)\n",
        "score = evaluate(dataset, metrics=metrics, llm=llm)\n",
        "\n",
        "# Print results and explanations\n",
        "results_df = score.to_pandas()\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyzing Metric Results from Ragas Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyse_metric_results(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interactive Chat Interface for Harry Potter Inquiries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV7UvEofS1nv"
      },
      "outputs": [],
      "source": [
        "def chat_with_data(chunks_retriever, chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm):\n",
        "    \"\"\"\n",
        "    Provides an interactive chat interface for answering questions about Harry Potter.\n",
        "\n",
        "    Args:\n",
        "        retriever: A retriever for retrieving relevant documents.\n",
        "        chapter_summaries_retriever: A retriever for retrieving relevant chapter summaries.\n",
        "        answer_from_context_llm_chain: An LLM chain for answering questions based on context.\n",
        "        multi_query_retriver_llm: An LLM for use in the MultiQueryRetriever.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"You can start chatting with me about Harry Potter. Type 'exit' to stop.\")\n",
        "\n",
        "    while True:\n",
        "        # Prompt the user for a question\n",
        "        question = input(\"What's your question? \\n\")\n",
        "\n",
        "        # Check if the user wants to exit\n",
        "        if question.lower() == 'exit':\n",
        "            print(\"Exiting chat. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Answer the question using the pipeline\n",
        "        result, _, _ = answer_question_pipeline(\n",
        "            question, chunks_retriever, chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm\n",
        "        )\n",
        "\n",
        "        # Print the answer\n",
        "        print(\"Answer:\")\n",
        "        wrapped_result = textwrap.fill(result['text'], width=120)  # Wrap text for readability\n",
        "        print(wrapped_result)\n",
        "        print(\"-\" * 80)  # Print a separator line for readability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calling the chat_with_data function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "048b2Ol7QAiF",
        "outputId": "1c891083-5c91-4425-ba23-50d1f32e0ac5"
      },
      "outputs": [],
      "source": [
        "chat_with_data(chunks_retriever,chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
