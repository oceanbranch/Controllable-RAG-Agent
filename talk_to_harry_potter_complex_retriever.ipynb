{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installing Libraries for LangChain and Supporting Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjDmS3Cn2jKm",
        "outputId": "569cfc27-0dbb-455a-85cb-3a75b57bbbf2"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install faiss-gpu\n",
        "!pip install langchain_experimental\n",
        "!pip install \"langchain[docarray]\"\n",
        "!pip install pylcs\n",
        "!pip3 install pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing Necessary Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XIL5OCqJ7bh8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\N7\\Anaconda3\\envs\\openai-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import  PyPDFLoader\n",
        "from langchain.vectorstores import  FAISS\n",
        "from langchain.text_splitter import  RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings \n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from time import monotonic\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import textwrap\n",
        "import os\n",
        "import ast\n",
        "from datasets import Dataset\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    answer_correctness,\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    answer_similarity\n",
        ")\n",
        "\n",
        "from helper_functions import num_tokens_from_string, replace_t_with_space, replace_double_lines_with_one_line, split_into_chapters, is_similarity_ratio_lower_than_th, analyse_metric_results\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting Preferred Encoding for PyPDF on Google Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6QeKRm37HwC"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding # For using PyPDF on google colab "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting OPENAI and GROQ API keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "groq_api_key = os.getenv('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Path to Harry Potter PDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8mUYHG_S6y22"
      },
      "outputs": [],
      "source": [
        "hp_pdf_path =\"Harry_Potter_Book_1_The_Sorcerers_Stone.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splitting the PDF into Chapters and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cDHfDODdTIBY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17\n"
          ]
        }
      ],
      "source": [
        "chapters = split_into_chapters(hp_pdf_path) \n",
        "chapters = replace_t_with_space(chapters)\n",
        "print(len(chapters))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Prompt Template for Summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "49RqsAhDxjFg"
      },
      "outputs": [],
      "source": [
        "summarization_prompt_template = \"\"\"Write an extensive summary of about of the following:\n",
        "\n",
        "{text}\n",
        "\n",
        "SUMMARY:\"\"\"\n",
        "\n",
        "summarization_prompt = PromptTemplate(template=summarization_prompt_template, input_variables=[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Function to Create Chapter Summaries using LLMs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ehe6iObnx4l9"
      },
      "outputs": [],
      "source": [
        "def create_chapter_summary(chapter):\n",
        "    \"\"\"\n",
        "    Creates a summary of a chapter using a large language model (LLM).\n",
        "\n",
        "    Args:\n",
        "        chapter: A Document object representing the chapter to summarize.\n",
        "\n",
        "    Returns:\n",
        "        A Document object containing the summary of the chapter.\n",
        "    \"\"\"\n",
        "\n",
        "    chapter_txt = chapter.page_content  # Extract chapter text\n",
        "    model_name = \"gpt-3.5-turbo-0125\"  # Specify LLM model\n",
        "    llm = ChatOpenAI(temperature=0, model_name=model_name)  # Create LLM instance\n",
        "    gpt_35_turbo_max_tokens = 16000  # Maximum token limit for the LLM\n",
        "    verbose = False  # Set to True for more detailed output\n",
        "\n",
        "    # Calculate number of tokens in the chapter text\n",
        "    num_tokens = num_tokens_from_string(chapter_txt, model_name)\n",
        "\n",
        "    # Choose appropriate chain type based on token count\n",
        "    if num_tokens < gpt_35_turbo_max_tokens:\n",
        "        chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=summarization_prompt, verbose=verbose)\n",
        "    else:\n",
        "        chain = load_summarize_chain(llm, chain_type=\"map_reduce\", map_prompt=summarization_prompt, combine_prompt=summarization_prompt, verbose=verbose)\n",
        "\n",
        "    start_time = monotonic()  # Start timer\n",
        "    doc_chapter = Document(page_content=chapter_txt)  # Create Document object for chapter\n",
        "    summary = chain.invoke([doc_chapter])  # Generate summary using the chain\n",
        "    print(f\"Chain type: {chain.__class__.__name__}\")  # Print chain type\n",
        "    print(f\"Run time: {monotonic() - start_time}\")  # Print execution time\n",
        "\n",
        "    # Clean up summary text\n",
        "    summary = replace_double_lines_with_one_line(summary[\"output_text\"])\n",
        "\n",
        "    # Create Document object for summary\n",
        "    doc_summary = Document(page_content=summary, metadata=chapter.metadata)\n",
        "\n",
        "    return doc_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Summaries for Each Chapter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4UBurLsMCHj",
        "outputId": "668932cf-02d8-446d-e0ed-3f85b30d3bba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\N7\\Anaconda3\\envs\\openai-env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chain type: StuffDocumentsChain\n",
            "Run time: 12.952999999979511\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 10.734999999869615\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 7.922000000020489\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 6.405999999959022\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 9.234000000171363\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 9.343999999808148\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 6.780999999959022\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 9.125\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 7.2350000001024455\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 9.546999999787658\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 7.765999999828637\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 6.7030000002123415\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 6.0\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 9.781999999890104\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 7.733999999938533\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 10.218000000109896\n",
            "Chain type: StuffDocumentsChain\n",
            "Run time: 6.547000000020489\n"
          ]
        }
      ],
      "source": [
        "chapter_summaries = []\n",
        "for chapter in chapters:\n",
        "    chapter_summaries.append(create_chapter_summary(chapter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function to Encode a Book into a Vector Store using OpenAI Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9t1Kq6fGS0EA"
      },
      "outputs": [],
      "source": [
        "def encode_book(path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        path: The path to the PDF file.\n",
        "        chunk_size: The desired size of each text chunk.\n",
        "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded book content.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load PDF documents\n",
        "    loader = PyPDFLoader(path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    cleaned_texts = replace_t_with_space(texts)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
        "\n",
        "    return vectorstore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding Chapter Summaries into Vector Store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FHt9Y12gMp2z"
      },
      "outputs": [],
      "source": [
        "def encode_chapter_summaries(chapter_summaries):\n",
        "    \"\"\"\n",
        "    Encodes a list of chapter summaries into a vector store using OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        chapter_summaries: A list of Document objects representing the chapter summaries.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded chapter summaries.\n",
        "    \"\"\"\n",
        "\n",
        "    embeddings = OpenAIEmbeddings()  # Create OpenAI embeddings\n",
        "    chapter_summaries_vectorstore = FAISS.from_documents(chapter_summaries, embeddings)  # Create vector store\n",
        "    return chapter_summaries_vectorstore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Vector Stores and Retrievers for Book and Chapter Summaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\N7\\Anaconda3\\envs\\openai-env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "# ### IF VECTOR STORES ALREADY EXIST, LOAD THEM\n",
        "if os.path.exists(\"chunks_vector_store\") and os.path.exists(\"chapter_summaries_vector_store\"):\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    chunks_vector_store =  FAISS.load_local(\"chunks_vector_store\", embeddings, allow_dangerous_deserialization=True)\n",
        "    chapter_summaries_vector_store =  FAISS.load_local(\"chapter_summaries_vector_store\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "else:\n",
        "    chunks_vector_store = encode_book(hp_pdf_path, chunk_size=1000, chunk_overlap=200)\n",
        "    chapter_summaries_vector_store = encode_chapter_summaries(chapter_summaries)\n",
        "\n",
        "    chunks_vector_store.save_local(\"chunks_vector_store\") # save the chunks_vector_store\n",
        "    chapter_summaries_vector_store.save_local(\"chapter_summaries_vector_store\") # save the chapter_summaries_vector_store\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create retrievers from the vector stores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunks_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 1})     \n",
        "chapter_summaries_retriever = chapter_summaries_vector_store.as_retriever(search_kwargs={\"k\": 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agrregate retrieved content as string context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_chunks_context_per_question(question, chunks_query_retriever):\n",
        " \n",
        "    # Retrieve relevant documents\n",
        "    docs = chunks_query_retriever.get_relevant_documents(question)\n",
        "\n",
        "    # Concatenate document content\n",
        "    context = \" \".join(doc.page_content for doc in docs)\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "\n",
        "def create_summaries_context_per_question(question, chapter_summaries_query_retriever):\n",
        "   \n",
        "    # Retrieve relevant chapter summaries\n",
        "    docs_summaries = chapter_summaries_query_retriever.get_relevant_documents(question)\n",
        "\n",
        "\n",
        "    # Concatenate chapter summaries with citation information\n",
        "    context_summaries = \"\".join(\n",
        "        f\"{doc.page_content} (Chapter {doc.metadata['chapter']})\" for doc in docs_summaries\n",
        "    )\n",
        "\n",
        "    return context_summaries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM based function to distill only relevant retrieved content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "keep_only_relevant_content_prompt_template = \"\"\"you receive a query: {query} and retrieved docuemnts: {retrieved_documents} from a vector store.\n",
        " You need to filter the retrieved data and keep only the sentences that are relevant, but all of them.\n",
        " you should output the distilled content in a json format. \n",
        " REMEMBER: the output has to be a json containing ALL the relevant sentences, and not the answer to the query. {format_instructions}\"\"\"\n",
        "\n",
        "class QuestionAnswer(BaseModel):\n",
        "    relevant_content: str = Field(description=\"The relevant content from the retrieved documents that is relevant to the query.\")\n",
        "\n",
        "\n",
        "json_parser = JsonOutputParser(pydantic_object=QuestionAnswer)\n",
        "\n",
        "keep_only_relevant_content_prompt = PromptTemplate(\n",
        "    template=keep_only_relevant_content_prompt_template,\n",
        "    input_variables=[\"query\", \"retrieved_documents\"],\n",
        "    partial_variables={\"format_instructions\": json_parser.get_format_instructions()}, \n",
        ")\n",
        "\n",
        "keep_only_relevant_content_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
        "keep_only_relevant_content_chain = keep_only_relevant_content_prompt | keep_only_relevant_content_llm | json_parser\n",
        "\n",
        "def keep_only_relevant_content(question, context, chain):\n",
        "    \"\"\"\n",
        "    Keeps only the relevant content from the retrieved documents that is relevant to the query.\n",
        "\n",
        "    Args:\n",
        "        question: The query question.\n",
        "        context: The retrieved documents.\n",
        "        chain: The LLMChain instance.\n",
        "\n",
        "    Returns:\n",
        "        The relevant content from the retrieved documents that is relevant to the query.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create Document objects for the query and retrieved documents\n",
        "    doc_query = Document(page_content=question)\n",
        "    doc_retrieved_documents = Document(page_content=context)\n",
        "\n",
        "    input_data = {\n",
        "    \"query\": doc_query,\n",
        "    \"retrieved_documents\": doc_retrieved_documents\n",
        "}\n",
        "    # Invoke the chain to keep only the relevant content\n",
        "    output = chain.invoke(input_data)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### combine retrival with content distilation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_relevant_chunks_per_question(question, chunks_retriever, keep_only_relevant_content_chain):\n",
        "    \"\"\"\n",
        "    Retrieves relevant chunks of text from the book based on a question.\n",
        "\n",
        "    Args:\n",
        "        question: The question to ask.\n",
        "        chunks_retriever: The retriever for the book chunks.\n",
        "        keep_only_relevant_content_chain: The chain to keep only the relevant content.\n",
        "\n",
        "    Returns:\n",
        "        The relevant chunks of text from the book based on the question.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the context for the question\n",
        "    context = create_chunks_context_per_question(question, chunks_retriever)\n",
        "\n",
        "    # Keep only the relevant content from the retrieved documents that is relevant to the query\n",
        "    relevant_content = keep_only_relevant_content(question, context, keep_only_relevant_content_chain)\n",
        "\n",
        "    return relevant_content\n",
        "\n",
        "\n",
        "def get_relevant_summaries_per_question(question, chapter_summaries_retriever, keep_only_relevant_content_chain):\n",
        "    \"\"\"\n",
        "    Retrieves relevant chapter summaries based on a question.\n",
        "\n",
        "    Args:\n",
        "        question: The question to ask.\n",
        "        chapter_summaries_retriever: The retriever for the chapter summaries.\n",
        "        keep_only_relevant_content_chain: The chain to keep only the relevant content.\n",
        "\n",
        "    Returns:\n",
        "        The relevant chapter summaries based on the question.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the context for the question\n",
        "    context_summaries = create_summaries_context_per_question(question, chapter_summaries_retriever)\n",
        "\n",
        "    # Keep only the relevant content from the retrieved documents that is relevant to the query\n",
        "    relevant_content_summaries = keep_only_relevant_content(question, context_summaries, keep_only_relevant_content_chain)\n",
        "\n",
        "    return relevant_content_summaries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM based function to determine if retrieved content is relevant to question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "is_relevant_content_prompt_template = \"\"\"you receive a query: {query} and a document: {document} from a vector store. \n",
        "You need to determine if the document is relevant to the query. {format_instructions}\"\"\"\n",
        "\n",
        "class Relevance(BaseModel):\n",
        "    is_relevant: bool = Field(description=\"Whether the document is relevant to the query.\")\n",
        "\n",
        "json_parser = JsonOutputParser(pydantic_object=Relevance)\n",
        "is_relevant_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
        "\n",
        "is_relevant_content_prompt = PromptTemplate(\n",
        "    template=is_relevant_content_prompt_template,\n",
        "    input_variables=[\"query\", \"document\"],\n",
        "    partial_variables={\"format_instructions\": json_parser.get_format_instructions()},\n",
        ")\n",
        "is_relevant_content_chain = is_relevant_content_prompt | is_relevant_llm | json_parser\n",
        "\n",
        "def is_relevant_content(question, document, chain):\n",
        "    \"\"\"\n",
        "    Determines if a document is relevant to a query.\n",
        "\n",
        "    Args:\n",
        "        question: The query question.\n",
        "        document: The document to evaluate.\n",
        "        chain: The LLMChain instance.\n",
        "\n",
        "    Returns:\n",
        "        Whether the document is relevant to the query.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create Document objects for the query and document\n",
        "    doc_query = Document(page_content=question)\n",
        "    doc_document = Document(page_content=document)\n",
        "\n",
        "    input_data = {\n",
        "    \"query\": doc_query,\n",
        "    \"document\": doc_document\n",
        "}\n",
        "\n",
        "    # Invoke the chain to determine if the document is relevant\n",
        "    output = chain.invoke(input_data)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM based function to re-write a question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Question Re-writer\n",
        "\n",
        "class RewriteQuestion(BaseModel):\n",
        "    \"\"\"\n",
        "    Output schema for the rewritten question.\n",
        "    \"\"\"\n",
        "    rewritten_question: str = Field(description=\"The improved question optimized for vectorstore retrieval.\")\n",
        "\n",
        "rewrite_question_string_parser = JsonOutputParser(pydantic_object=RewriteQuestion)\n",
        "\n",
        "\n",
        "rewrite_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
        "rewrite_prompt_template = \"\"\"You are a question re-writer that converts an input question to a better version optimized for vectorstore retrieval.\n",
        " Analyze the input question {question} and try to reason about the underlying semantic intent / meaning.\n",
        " {format_instructions}\n",
        " \"\"\"\n",
        "\n",
        "# Prompt\n",
        "\n",
        "rewrite_prompt = PromptTemplate(\n",
        "    template=rewrite_prompt_template,\n",
        "    input_variables=[\"question\"],\n",
        "    partial_variables={\"format_instructions\": rewrite_question_string_parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "question_rewriter = rewrite_prompt | rewrite_llm | rewrite_question_string_parser  # Combine prompt, LLM, and parser\n",
        "\n",
        "def rewrite_question(question):\n",
        "    \"\"\"Rewrites the given question using the LLM.\"\"\"\n",
        "    result = question_rewriter.invoke({\"question\": question})\n",
        "    return result \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM based function to answer a question given context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QuestionAnswerFromContext(BaseModel):\n",
        "    relevant_content: str = Field(description=\"answer a question from a given context.\")\n",
        "\n",
        "question_answer_from_context_json_parser = JsonOutputParser(pydantic_object=QuestionAnswerFromContext)\n",
        "question_answer_from_context_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
        "question_answer_from_context_prompt_template = \"\"\"you receive a query: {query} and a context: {context} from a vector store. \n",
        "You need to answer the query from the context. {format_instructions}\"\"\"\n",
        "\n",
        "question_answer_from_context_prompt = PromptTemplate(\n",
        "    template=question_answer_from_context_prompt_template,\n",
        "    input_variables=[\"query\", \"context\"],\n",
        "    partial_variables={\"format_instructions\": question_answer_from_context_json_parser.get_format_instructions()},\n",
        ")\n",
        "question_answer_from_context_chain = question_answer_from_context_prompt | question_answer_from_context_llm | question_answer_from_context_json_parser\n",
        "\n",
        "def answer_question_from_context(question, context, chain):\n",
        "    \"\"\"\n",
        "    Answers a question from a given context.\n",
        "\n",
        "    Args:\n",
        "        question: The query question.\n",
        "        context: The context to answer the question from.\n",
        "        chain: The LLMChain instance.\n",
        "\n",
        "    Returns:\n",
        "        The answer to the question from the context.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create Document objects for the query and context\n",
        "    doc_query = Document(page_content=question)\n",
        "    doc_context = Document(page_content=context)\n",
        "\n",
        "    input_data = {\n",
        "    \"query\": doc_query,\n",
        "    \"context\": doc_context\n",
        "}\n",
        "\n",
        "    # Invoke the chain to answer the question from the context\n",
        "    output = chain.invoke(input_data)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM based function to check if an answer is hallucination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class is_hallucination(BaseModel):\n",
        "    \"\"\"\n",
        "    Output schema for the rewritten question.\n",
        "    \"\"\"\n",
        "    is_hallucination: bool = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n",
        "hallucination_parser = JsonOutputParser(pydantic_object=is_hallucination)\n",
        "is_hallucination_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
        "is_hallucination_prompt_template = \"\"\"You are a fact-checker that determines if the answer to the question is grounded in the facts.\n",
        " Analyze the input question {question} and the answer {answer} and determine if the answer is grounded in the facts.\n",
        " {format_instructions}\n",
        " \"\"\"\n",
        "is_hallucination_prompt = PromptTemplate(\n",
        "    template=is_hallucination_prompt_template,\n",
        "    input_variables=[\"question\", \"answer\"],\n",
        "    partial_variables={\"format_instructions\": hallucination_parser.get_format_instructions()},\n",
        ")\n",
        "is_hallucination_chain = is_hallucination_prompt | is_hallucination_llm | hallucination_parser\n",
        "\n",
        "def is_answer_hallucination(question, answer):\n",
        "    \"\"\"Determines if the answer to the question is grounded in the facts.\"\"\"\n",
        "    result = is_hallucination_chain.invoke({\"question\": question, \"answer\": answer})\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM based function to determine if a question can be fully answered given a context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "can_be_answered_prompt_template = \"\"\"You receive a query: {question} and a context: {context}. \n",
        "You need to determine if the question can be fully answered based on the context.\n",
        "{format_instructions}\n",
        "\n",
        "**Answer:**\n",
        "\"\"\"\n",
        "\n",
        "class QuestionAnswer(BaseModel):\n",
        "    can_be_answered: bool = Field(description=\"binary result of whether the question can be fully answered or not\")\n",
        "\n",
        "can_be_answered_json_parser = JsonOutputParser(pydantic_object=QuestionAnswer)\n",
        "\n",
        "answer_question_prompt = PromptTemplate(\n",
        "    template=can_be_answered_prompt_template,\n",
        "    input_variables=[\"question\",\"context\"],\n",
        "    partial_variables={\"format_instructions\": can_be_answered_json_parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "can_be_answered_llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
        "can_be_answered_chain = answer_question_prompt | can_be_answered_llm | can_be_answered_json_parser\n",
        "\n",
        "\n",
        "def can_be_answered_from_contenxt(question, context, can_be_answered_chain):\n",
        "    \"\"\"\n",
        "    Determines if a question can be answered based on the context.\n",
        "\n",
        "    Args:\n",
        "        question: The query question.\n",
        "        context: The retrieved documents.\n",
        "        chain: The LLMChain instance.\n",
        "\n",
        "    Returns:\n",
        "        A binary result of whether the question can be answered or not.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create Document objects for the query and context\n",
        "    doc_question = Document(page_content=question)\n",
        "    doc_context = Document(page_content=context)\n",
        "\n",
        "    input_data = {\n",
        "        \"question\": doc_question,\n",
        "        \"context\": doc_context\n",
        "    }\n",
        "\n",
        "    # Invoke the chain to determine if the question can be answered\n",
        "    output = can_be_answered_chain.invoke(input_data)\n",
        "\n",
        "    return output\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\N7\\Anaconda3\\envs\\openai-env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.memory import ConversationBufferWindowMemory, ConversationSummaryMemory\n",
        "\n",
        "\n",
        "llm_retrieval_agent = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key, max_tokens=4000)\n",
        "\n",
        "\n",
        "# # Craft agent description emphasizing reliance on retrieval\n",
        "agent_description = \"\"\"\n",
        "You are an AI assistant tasked with answering questions about the content of a book which is encoded into vector stores. you have both chunks encoded and chapter summaries encoded.\n",
        "You have no prior knowledge of any book, including characters, places, or events. \n",
        "Your answers must be based solely on the information you retrieve using the provided tools.\n",
        "You can use the RetrieveBookContent tool to search for specific details within the book content, and the Retrievechaptersummary tool to get a high-level overview of events and key points from the chapter summaries.\n",
        "every answer you provide should be supported by a *CITED* evidence from the book content or chapter summaries.\n",
        "if no relevant information is found, you should indicate that you were unable to find any relevant information.\n",
        "you must also determine if a question can be answered based on the context provided.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "tools = [\n",
        "Tool(\n",
        "    name=\"RetrieveBookContent\",\n",
        "    func=lambda question: get_relevant_chunks_per_question(question, chunks_retriever, keep_only_relevant_content_chain),\n",
        "    description=\"Retrieves relevant chunks of text from the book based on a question.\",\n",
        "),\n",
        "Tool(\n",
        "    name=\"RetrieveChapterSummary\",\n",
        "    func=lambda question: get_relevant_summaries_per_question(question, chapter_summaries_retriever, keep_only_relevant_content_chain),\n",
        "    description=\"Retrieves relevant chapter summaries based on a question.\",\n",
        "),\n",
        "# Tool(\n",
        "#     name=\"CanBeAnswered\",\n",
        "#     func=lambda question, context: can_be_answered_from_contenxt(question, context, can_be_answered_chain),\n",
        "#     description=\"Determines if a question can be answered based on the context.\",\n",
        "# ),\n",
        "]\n",
        "\n",
        "\n",
        "# Initialize QA Chain and Memory\n",
        "chain = load_qa_chain(llm_retrieval_agent, chain_type=\"stuff\", verbose=True)  # Properly initialize the QA chain\n",
        "# memory = ConversationBufferWindowMemory(k=2, return_messages=True)\n",
        "\n",
        "# Initialize Agent with QA Chain\n",
        "agent = initialize_agent(\n",
        "    tools, \n",
        "    llm_retrieval_agent, \n",
        "    agent=\"zero-shot-react-description\", \n",
        "    verbose=True,\n",
        "    agent_description=agent_description,\n",
        "    # max_iterations=7,\n",
        "    # memory=memory,\n",
        "    handle_parsing_errors=True,\n",
        "    chain=chain,  # Include the QA chain in agent initialization\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find out how Harry beat Quirrell in the first book.\n",
            "\n",
            "Action: RetrieveBookContent\n",
            "Action Input: \"Harry\" and \"Quirrell\" and \"final battle\"\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m{'relevant_content': 'Harry went on feverishly, “then Voldemort will be able to come and finish me off ... Well, I suppose Bane’ll be happy.”'}\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: It seems like the retrieved content is not directly answering my question. I need to find more information about the final battle between Harry and Quirrell.\n",
            "\n",
            "Action: RetrieveBookContent\n",
            "Action Input: \"Harry\" and \"Quirrell\" and \"final battle\" and \"fire\"\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m{'relevant_content': ['A lamp flickered on. It was Hermione Granger, wearing a pink bathrobe and a frown.', 'Harry couldn’t believe anyone could be so interfering.', '“You!” said Ron furiously. “Go back to bed!”', '“I almost told your brother,” Hermione snapped, “Percy — he’s a prefect, he’d put a stop to this.”', 'Harry couldn’t believe anyone could be so interfering.']}\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: It seems like the retrieved content is still not directly answering my question. I need to find more information about the final battle between Harry and Quirrell.\n",
            "\n",
            "Action: RetrieveChapterSummary\n",
            "Action Input: \"Chapter 17\" (assuming the final battle is in Chapter 17)\n",
            "\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m{'relevant_content': ['In Chapter Seventeen, titled \"The Man with Two Faces,\" Harry comes face to face with Professor Quirrell, who reveals himself to be the one trying to steal the Sorcerer’s Stone.', 'Quirrell explains how he tried to kill Harry during the Quidditch match and how Snape was actually trying to save him.', 'Quirrell also reveals that he is serving Lord Voldemort and that Voldemort is sharing his body.', 'Harry manages to get the Stone from the Mirror of Erised by lying to Quirrell and keeping him distracted.', 'Quirrell’s true form is revealed to be Voldemort’s face on the back of his head, and he tries to kill Harry but is unable to touch him due to the protection of his mother’s love.', 'Dumbledore arrives in time to save Harry and stop Quirrell.', 'The Stone is destroyed, and Dumbledore explains that Nicolas Flamel and his wife will die as a result.']}\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I finally have the information I need to answer the question. It seems that Harry didn't directly \"beat\" Quirrell, but rather, Quirrell was unable to touch Harry due to the protection of his mother's love. Additionally, Dumbledore arrived in time to save Harry and stop Quirrell.\n",
            "\n",
            "Final Answer: Harry didn't directly beat Quirrell, but rather, Quirrell was unable to touch Harry due to the protection of his mother's love, and Dumbledore arrived in time to save Harry and stop Quirrell.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "query = \"how did harry beat quirrell?\"\n",
        "result = agent.invoke(query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Sub-Questions for Vector Store Queries from User Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpVkNzd1hs8v"
      },
      "outputs": [],
      "source": [
        "modifier_prompt_template = \"\"\"\n",
        "You are an expert on analyzing and understanding books based solely on their textual content. You have no prior knowledge about the specific book in the question.\n",
        "If you ever read the related book before, FORGET ANYTHING about it.\n",
        "You have access to two vector stores:\n",
        "\n",
        "• One containing all the book content divided into chunks of 1000 characters with 200 character overlap.\n",
        "• Another containing summaries of each chapter, approximately 250 tokens each.\n",
        "\n",
        "Given a user's question about the book, your task is to generate a list of no more than 3 sub-questions that can be used as queries to retrieve relevant information from the vector stores based on semantic similarity. These sub-questions should be designed to collectively cover all the information needed to answer the original question comprehensively, without relying on any prior knowledge of the book's plot, characters, or events.\n",
        "\n",
        "When generating the sub-questions, consider the following:\n",
        "\n",
        "\n",
        "1.No Pre-knowledge: you're unaware of the book's details like plot or characters. The sub-questions must not present any prior knowledge of the book.\n",
        "2.Directly Derived: Create sub-questions strictly from the user's question, aiming to retrieve book information without presupposing specific plot details or events.\n",
        "3.Break Down: Decompose the user's question into finer, detailed sub-questions using only the textual content provided.\n",
        "4.Key Concepts: Identify essential information needed from the original question and form targeted sub-questions to gather this data.\n",
        "5.Self-contained Queries: Each sub-question should stand alone for effective vector store querying through semantic similarity.\n",
        "6.Logical Sequence: Arrange sub-questions in a logical order that collectively provides a thorough answer.\n",
        "7.Efficiency: Ensure sub-questions are unique and focused to avoid redundant searches and streamline information retrieval.\n",
        "\n",
        "Output your response as a Python list, where each item is a self-contained sub-question that can be used as a standalone query for vector retrieval.\n",
        "\n",
        "User's question: {question}\n",
        "\"\"\"\n",
        "modifier_prompt = PromptTemplate(\n",
        "input_variables=[\"question\"],\n",
        "template=modifier_prompt_template,\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-1106-preview\", max_tokens=4000)\n",
        "\n",
        "question_modifier_llm_chain = LLMChain(llm=llm, prompt=modifier_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modifying and Cleaning User Questions for Enhanced Querying\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwSLdGdb1FiO"
      },
      "outputs": [],
      "source": [
        "def modify_question(question):\n",
        "    \"\"\"\n",
        "    Modifies a question using a question modifier LLM chain and cleans up the output.\n",
        "\n",
        "    Args:\n",
        "        question: The input question string.\n",
        "\n",
        "    Returns:\n",
        "        A list of modified questions, or an empty list if there's an error.\n",
        "    \"\"\"\n",
        "\n",
        "    # Invoke the question modifier LLM chain\n",
        "    result = question_modifier_llm_chain.invoke(input=question)\n",
        "    modified_questions_str = result['text']\n",
        "\n",
        "    # Clean up the output string\n",
        "    clean_str = modified_questions_str.replace(\"```python\", \"\").replace(\"```\", \"\").strip()  # Remove Markdown code block syntax\n",
        "    clean_str = clean_str.replace(\"sub_questions = \", \"\").strip()  # Remove variable assignment\n",
        "    clean_str = textwrap.dedent(clean_str)  # Dedent to remove unexpected indents\n",
        "\n",
        "    # Attempt to convert the cleaned string into a list of questions\n",
        "    try:\n",
        "        modified_questions = ast.literal_eval(clean_str)\n",
        "    except SyntaxError as e:\n",
        "        print(f\"Syntax error during ast.literal_eval: {e}\")\n",
        "        modified_questions = []  # Default to an empty list in case of error\n",
        "\n",
        "    return modified_questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Generating Modified Questions from an Original Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXfrK0m5DUq8"
      },
      "outputs": [],
      "source": [
        "question = \"how did harry beat quirrell?\"\n",
        "modified_question = modify_question(question)\n",
        "print(modified_question) # watch new questions generated based on the original question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Context for a Question by Retrieving Relevant Documents and Summaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEFLePy-0YS3"
      },
      "outputs": [],
      "source": [
        "def create_context_per_question(question, multi_query_retriever, multi_query_retriever_chapter_summaries):\n",
        "    \"\"\"\n",
        "    Creates context for a question by retrieving relevant documents and chapter summaries.\n",
        "\n",
        "    Args:\n",
        "        question: The input question string.\n",
        "        multi_query_retriever: A retriever for retrieving relevant documents.\n",
        "        multi_query_retriever_chapter_summaries: A retriever for retrieving relevant chapter summaries.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing two strings:\n",
        "            - context: The concatenated content of relevant documents.\n",
        "            - context_summaries: The concatenated content of relevant chapter summaries with citation information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve relevant documents and chapter summaries\n",
        "    docs = multi_query_retriever.get_relevant_documents(question)\n",
        "    docs_summaries = multi_query_retriever_chapter_summaries.get_relevant_documents(question)\n",
        "\n",
        "    # Concatenate document content\n",
        "    context = \" \".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # Concatenate chapter summaries with citation information\n",
        "    context_summaries = \"\".join(\n",
        "        f\"{doc.page_content} (Chapter {doc.metadata['chapter']})\" for doc in docs_summaries\n",
        "    )\n",
        "\n",
        "    return context, context_summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comprehensive Question Answering Pipeline Using Context Retrieval and LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjk7u5ME7ZzX"
      },
      "outputs": [],
      "source": [
        "def answer_question_pipeline(question, chunks_retriever, chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm, similarity_th=0.5):\n",
        "    \"\"\"\n",
        "    Answers a question by retrieving relevant context, modifying the question, and using an LLM chain.\n",
        "\n",
        "    Args:\n",
        "        question: The input question string.\n",
        "        retriever: A retriever for retrieving relevant documents.\n",
        "        chapter_summaries_retriever: A retriever for retrieving relevant chapter summaries.\n",
        "        answer_from_context_llm_chain: An LLM chain for answering questions based on context.\n",
        "        multi_query_retriver_llm: An LLM for use in the MultiQueryRetriever.\n",
        "        similarity_th:  Similarity threshold for context accumulation\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            - result: The result of invoking the answer_from_context_llm_chain.\n",
        "            - all_context_book: The concatenated content of relevant documents.\n",
        "            - all_context_summaries: The concatenated content of relevant chapter summaries.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create MultiQueryRetrievers\n",
        "    multi_query_retriever = MultiQueryRetriever.from_llm(retriever=chunks_retriever, llm=multi_query_retriver_llm)\n",
        "    multi_query_retriever_chapter_summaries = MultiQueryRetriever.from_llm(retriever=chapter_summaries_retriever, llm=multi_query_retriver_llm)\n",
        "\n",
        "    # Modify the question\n",
        "    modified_questions = modify_question(question)\n",
        "\n",
        "    # Accumulate relevant context\n",
        "    all_context_book = \"\"\n",
        "    all_context_summaries = \"\"\n",
        "\n",
        "    for modified_question in modified_questions:\n",
        "        curr_question_relevant_context, curr_question_relevant_summaries_context = create_context_per_question(\n",
        "            modified_question, multi_query_retriever, multi_query_retriever_chapter_summaries\n",
        "        )\n",
        "\n",
        "        # Add context if it's not too similar to existing context\n",
        "        if is_similarity_ratio_lower_than_th(all_context_book, curr_question_relevant_context, similarity_th):\n",
        "            all_context_book += curr_question_relevant_context\n",
        "\n",
        "        if is_similarity_ratio_lower_than_th(all_context_summaries, curr_question_relevant_summaries_context, similarity_th):\n",
        "            all_context_summaries += curr_question_relevant_summaries_context\n",
        "\n",
        "    # Combine context from book and summaries\n",
        "    all_context = all_context_book + all_context_summaries\n",
        "\n",
        "    # Prepare input data for the LLM chain\n",
        "    input_data = {\n",
        "        \"context\": all_context,\n",
        "        \"question\": question\n",
        "    }\n",
        "\n",
        "    # Execute the LLM chain and get the response\n",
        "    result = answer_from_context_llm_chain.invoke(input=input_data)\n",
        "\n",
        "    return result, all_context_book, all_context_summaries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Template for Answering Questions Using Context-Specific Information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV6v5-ajP_i-"
      },
      "outputs": [],
      "source": [
        "answer_question_prompt_template = \"\"\"\n",
        "Based solely on the information provided in this context, and without using any information outside of this context, please answer the following question as concisely and as shortly as possible. You can rephrase the question for better fitting to the context.\n",
        "\n",
        "Context:{context}\n",
        "Question:{question}\n",
        "\n",
        "**If the answer cannot be derived from the context, or if it requires knowledge from outside sources, simply answer: \"I don't know\".**\n",
        "\n",
        "Please cite specific parts of the context in your answer to demonstrate how it supports your response.\n",
        "If the chapter number of the relevant context appears, specify it in your answer.\n",
        "\"\"\"\n",
        "answer_question_prompt = PromptTemplate(\n",
        "input_variables=[\"context\", \"question\"],\n",
        "template=answer_question_prompt_template,\n",
        ")\n",
        "\n",
        "multi_query_retriver_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-1106-preview\", max_tokens=4000)\n",
        "\n",
        "answer_from_context_llm_chain = LLMChain(llm=llm, prompt=answer_question_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example Execution of the Question Answering Pipeline for \"How Did Harry Beat Quirrell?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YlJOyrULV5c"
      },
      "outputs": [],
      "source": [
        "question = 'how did harry beat quirrell?'\n",
        "result, all_context_book, all_context_summaries = answer_question_pipeline(question,chunks_retriever,chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm)\n",
        "\n",
        "wrapped_all_context_book = textwrap.fill(all_context_book, width=120)\n",
        "print(f' conetxt book: {wrapped_all_context_book} \\n')\n",
        "\n",
        "wrapped_all_context_summaries = textwrap.fill(all_context_summaries, width=120)\n",
        "print(f' context summaries: {wrapped_all_context_summaries} \\n')\n",
        "\n",
        "wrapped_result = textwrap.fill(result['text'], width=120)\n",
        "print(f' answer: {wrapped_result}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBKQkR-W4ZyV"
      },
      "source": [
        "### Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME52zvjPNv47"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"Who gave Harry Potter his first broomstick?\",\n",
        "    \"What is the name of the three-headed dog guarding the Sorcerer's Stone?\",\n",
        "    \"Which house did the Sorting Hat initially consider for Harry?\",\n",
        "    \"What is the name of Harry's owl?\"\n",
        "]\n",
        "#     \"How did Harry and his friends get past Fluffy?\",\n",
        "#     \"What is the Mirror of Erised?\",\n",
        "#     \"Who tried to steal the Sorcerer's Stone?\",\n",
        "#     \"How did Harry defeat Quirrell/Voldemort?\",\n",
        "#     \"What is Harry's parent's secret weapon against Voldemort?\",\n",
        "# ]\n",
        "\n",
        "ground_truth_answers = [\n",
        "    \"Professor McGonagall\",\n",
        "    \"Fluffy\",\n",
        "    \"Slytherin\",\n",
        "    \"Hedwig\",\n",
        "    # \"They played music to put Fluffy to sleep.\",\n",
        "    # \"A magical mirror that shows the 'deepest, most desperate desire of our hearts.'\",\n",
        "    # \"Professor Quirrell, possessed by Voldemort\",\n",
        "    # \"Harry's mother's love protected him, causing Quirrell/Voldemort pain when they touched him.\",\n",
        "    # \"Love\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Answers and Retrieving Documents for Predefined Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "Wo9EEV0j4mJA",
        "outputId": "14c46214-8fb8-41aa-9845-73367d9a738f"
      },
      "outputs": [],
      "source": [
        "generated_answers = []\n",
        "retrieved_documents = []\n",
        "for question in questions:\n",
        "    result, all_context_book, all_context_summaries = answer_question_pipeline(question, chunks_retriever, chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm)\n",
        "    generated_answers.append(result['text'])\n",
        "    retrieved_documents.append(all_context_book + all_context_summaries)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Displaying Retrieved Documents and Generated Answers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'retrieved_documents: {retrieved_documents}\\n')\n",
        "print(f'generated_answers: {generated_answers}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing Data and Conducting Ragas Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for Ragas evaluation\n",
        "data_samples = {\n",
        "    'question': questions,  # Replace with your list of questions\n",
        "    'answer': generated_answers,  # Replace with your list of generated answers\n",
        "    'contexts': retrieved_documents,  # Your retrieved_documents list\n",
        "    'ground_truth': ground_truth_answers  # Replace with your list of ground truth answers\n",
        "}\n",
        "\n",
        "# Convert contexts to list of strings (if necessary)\n",
        "data_samples['contexts'] = [list(context) for context in data_samples['contexts']]\n",
        "\n",
        "dataset = Dataset.from_dict(data_samples)\n",
        "\n",
        "# Evaluate using Ragas with the specified metrics\n",
        "metrics = [\n",
        "    answer_correctness,\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    answer_similarity\n",
        "]\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-1106-preview\", max_tokens=4000)\n",
        "score = evaluate(dataset, metrics=metrics, llm=llm)\n",
        "\n",
        "# Print results and explanations\n",
        "results_df = score.to_pandas()\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyzing Metric Results from Ragas Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyse_metric_results(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interactive Chat Interface for Harry Potter Inquiries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV7UvEofS1nv"
      },
      "outputs": [],
      "source": [
        "def chat_with_data(chunks_retriever, chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm):\n",
        "    \"\"\"\n",
        "    Provides an interactive chat interface for answering questions about Harry Potter.\n",
        "\n",
        "    Args:\n",
        "        retriever: A retriever for retrieving relevant documents.\n",
        "        chapter_summaries_retriever: A retriever for retrieving relevant chapter summaries.\n",
        "        answer_from_context_llm_chain: An LLM chain for answering questions based on context.\n",
        "        multi_query_retriver_llm: An LLM for use in the MultiQueryRetriever.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"You can start chatting with me about Harry Potter. Type 'exit' to stop.\")\n",
        "\n",
        "    while True:\n",
        "        # Prompt the user for a question\n",
        "        question = input(\"What's your question? \\n\")\n",
        "\n",
        "        # Check if the user wants to exit\n",
        "        if question.lower() == 'exit':\n",
        "            print(\"Exiting chat. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Answer the question using the pipeline\n",
        "        result, _, _ = answer_question_pipeline(\n",
        "            question, chunks_retriever, chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm\n",
        "        )\n",
        "\n",
        "        # Print the answer\n",
        "        print(\"Answer:\")\n",
        "        wrapped_result = textwrap.fill(result['text'], width=120)  # Wrap text for readability\n",
        "        print(wrapped_result)\n",
        "        print(\"-\" * 80)  # Print a separator line for readability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calling the chat_with_data function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "048b2Ol7QAiF",
        "outputId": "1c891083-5c91-4425-ba23-50d1f32e0ac5"
      },
      "outputs": [],
      "source": [
        "chat_with_data(chunks_retriever,chapter_summaries_retriever, answer_from_context_llm_chain, multi_query_retriver_llm)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
